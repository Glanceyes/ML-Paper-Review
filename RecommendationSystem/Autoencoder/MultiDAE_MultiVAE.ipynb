{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import bottleneck as bn\n",
    "import adabound\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from scipy import sparse\n",
    "from copy import deepcopy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-DAE & Multi-VAE with MovieLens20M\n",
    "Multi-DAE와 Multi-VAE를 사용하여 MovieLens20M 데이터를 기반으로 한 유저별 영화 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 각종 파라미터 세팅\n",
    "parser = argparse.ArgumentParser(description='PyTorch Variational Autoencoders for Collaborative Filtering')\n",
    "\n",
    "\n",
    "parser.add_argument('--data', type=str, default='/opt/ml/input/data/train/',\n",
    "                    help='Movielens dataset location')\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--wd', type=float, default=0.00,\n",
    "                    help='weight decay coefficient')\n",
    "parser.add_argument('--batch_size', type=int, default=500,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=50, #원래 20\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--total_anneal_steps', type=int, default=200000,\n",
    "                    help='the total number of gradient updates for annealing')\n",
    "parser.add_argument('--anneal_cap', type=float, default=0.2,\n",
    "                    help='largest annealing parameter')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--cuda', action='store_true',\n",
    "                    help='use CUDA')\n",
    "parser.add_argument('--log_interval', type=int, default=100, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--dae_save', type=str, default='best_dae_model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--vae_save', type=str, default='best_vae_model.pt',\n",
    "                    help='path to save the final model')\n",
    "parser.add_argument('--recvae_save', type=str, default='best_model.pt',\n",
    "                    help='path to save the final model')\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed manually for reproductibility.\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "#만약 GPU가 사용가능한 환경이라면 GPU를 사용\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    if min_sc > 0:\n",
    "        itemcount = get_count(tp, 'item')\n",
    "        tp = tp[tp['item'].isin(itemcount.index[itemcount >= min_sc])]\n",
    "\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'user')\n",
    "        tp = tp[tp['user'].isin(usercount.index[usercount >= min_uc])]\n",
    "\n",
    "    usercount, itemcount = get_count(tp, 'user'), get_count(tp, 'item')\n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2): #원래 0.2\n",
    "    data_grouped_by_user = data.groupby('user')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(98765)\n",
    "\n",
    "    for _, group in data_grouped_by_user:\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "\n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def numerize(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load and Preprocess Movielens dataset\n",
      "원본 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "print(\"Load and Preprocess Movielens dataset\")\n",
    "DATA_DIR = args.data\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'train_ratings.csv'), header=0)\n",
    "print(\"원본 데이터\\n\", raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\n",
      "            user   item        time\n",
      "0            11   4643  1230782529\n",
      "1            11    170  1230782534\n",
      "2            11    531  1230782539\n",
      "3            11    616  1230782542\n",
      "4            11   2140  1230782563\n",
      "...         ...    ...         ...\n",
      "5154466  138493  44022  1260209449\n",
      "5154467  138493   4958  1260209482\n",
      "5154468  138493  68319  1260209720\n",
      "5154469  138493  40819  1260209726\n",
      "5154470  138493  27311  1260209807\n",
      "\n",
      "[5154471 rows x 3 columns]\n",
      "유저별 리뷰수\n",
      " user\n",
      "11        376\n",
      "14        180\n",
      "18         77\n",
      "25         91\n",
      "31        154\n",
      "         ... \n",
      "138473     63\n",
      "138475    124\n",
      "138486    137\n",
      "138492     68\n",
      "138493    314\n",
      "Length: 31360, dtype: int64\n",
      "아이템별 리뷰수\n",
      " item\n",
      "1         12217\n",
      "2          3364\n",
      "3           734\n",
      "4            43\n",
      "5           590\n",
      "          ...  \n",
      "118700       54\n",
      "118900       60\n",
      "118997       52\n",
      "119141      122\n",
      "119145       78\n",
      "Length: 6807, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter Data\n",
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data, min_uc=5, min_sc=0)\n",
    "#제공된 훈련데이터의 유저는 모두 5개 이상의 리뷰가 있습니다.\n",
    "print(\"5번 이상의 리뷰가 있는 유저들로만 구성된 데이터\\n\",raw_data)\n",
    "\n",
    "print(\"유저별 리뷰수\\n\",user_activity)\n",
    "print(\"아이템별 리뷰수\\n\",item_popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(BEFORE) unique_uid: Int64Index([    11,     14,     18,     25,     31,     35,     43,     50,\n",
      "                58,     60,\n",
      "            ...\n",
      "            138459, 138461, 138470, 138471, 138472, 138473, 138475, 138486,\n",
      "            138492, 138493],\n",
      "           dtype='int64', name='user', length=31360)\n",
      "(AFTER) unique_uid: Int64Index([ 27968,  67764,   2581,  82969, 137831,  48639,  97870,  40424,\n",
      "             46835,  79570,\n",
      "            ...\n",
      "            114284,   9009,  21165,  33920,  22054, 135379, 125855,  41891,\n",
      "             15720,  17029],\n",
      "           dtype='int64', name='user', length=31360)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle User Indices\n",
    "unique_uid = user_activity.index\n",
    "print(\"(BEFORE) unique_uid:\", unique_uid)\n",
    "np.random.seed(98765)\n",
    "idx_perm = np.random.permutation(unique_uid.size)\n",
    "unique_uid = unique_uid[idx_perm]\n",
    "print(\"(AFTER) unique_uid:\",unique_uid)\n",
    "\n",
    "n_users = unique_uid.size #31360\n",
    "n_heldout_users = 3136 #3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터에 사용될 사용자 수: 25088\n",
      "검증 데이터에 사용될 사용자 수: 3136\n",
      "테스트 데이터에 사용될 사용자 수: 3136\n"
     ]
    }
   ],
   "source": [
    "# Split Train/Validation/Test User Indices\n",
    "tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
    "vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
    "te_users = unique_uid[(n_users - n_heldout_users):]\n",
    "\n",
    "#주의: 데이터의 수가 아닌 사용자의 수입니다.\n",
    "print(\"훈련 데이터에 사용될 사용자 수:\", len(tr_users))\n",
    "print(\"검증 데이터에 사용될 사용자 수:\", len(vd_users))\n",
    "print(\"테스트 데이터에 사용될 사용자 수:\", len(te_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##훈련 데이터에 해당하는 아이템들\n",
    "#Train에는 전체 데이터를 사용합니다.\n",
    "train_plays = raw_data.loc[raw_data['user'].isin(tr_users)]\n",
    "\n",
    "##아이템 ID\n",
    "unique_sid = pd.unique(train_plays['item'])\n",
    "\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
    "\n",
    "pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
    "\n",
    "if not os.path.exists(pro_dir):\n",
    "    os.makedirs(pro_dir)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
    "    for uid in list(unique_uid):\n",
    "        f.write('%s\\n' % uid)\n",
    "\n",
    "with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31360\n"
     ]
    }
   ],
   "source": [
    "print(len(list(unique_uid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Validation과 Test에는 input으로 사용될 tr 데이터와 정답을 확인하기 위한 te 데이터로 분리되었습니다.\n",
    "vad_plays = raw_data.loc[raw_data['user'].isin(vd_users)]\n",
    "vad_plays = vad_plays.loc[vad_plays['item'].isin(unique_sid)]\n",
    "vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
    "\n",
    "test_plays = raw_data.loc[raw_data['user'].isin(te_users)]\n",
    "test_plays = test_plays.loc[test_plays['item'].isin(unique_sid)]\n",
    "test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_data = numerize(train_plays, profile2id, show2id)\n",
    "train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
    "\n",
    "\n",
    "vad_data_tr = numerize(vad_plays_tr, profile2id, show2id)\n",
    "vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
    "\n",
    "vad_data_te = numerize(vad_plays_te, profile2id, show2id)\n",
    "vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
    "\n",
    "test_data_tr = numerize(test_plays_tr, profile2id, show2id)\n",
    "test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
    "\n",
    "test_data_te = numerize(test_plays_te, profile2id, show2id)\n",
    "test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
    "\n",
    "check_plays_tr, check_plays_te = split_train_test_proportion(raw_data)\n",
    "check_data_tr = numerize(check_plays_tr, profile2id, show2id)\n",
    "check_data_tr.to_csv(os.path.join(pro_dir, 'check_tr.csv'), index=False)\n",
    "\n",
    "check_data_te = numerize(check_plays_te, profile2id, show2id)\n",
    "check_data_te.to_csv(os.path.join(pro_dir, 'check_te.csv'), index=False)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           uid   sid\n",
      "0        11825     0\n",
      "1        11825     1\n",
      "2        11825     2\n",
      "3        11825     3\n",
      "4        11825     4\n",
      "...        ...   ...\n",
      "5154466  10783   477\n",
      "5154467  10783  1325\n",
      "5154468  10783   331\n",
      "5154469  10783   558\n",
      "5154470  10783  1922\n",
      "\n",
      "[4125303 rows x 2 columns]\n",
      "           uid   sid\n",
      "376      26554   440\n",
      "377      26554   741\n",
      "378      26554  1407\n",
      "379      26554   193\n",
      "380      26554  1041\n",
      "...        ...   ...\n",
      "5153247  26934   760\n",
      "5153248  26934   697\n",
      "5153249  26934  3232\n",
      "5153250  26934  1369\n",
      "5153251  26934  3679\n",
      "\n",
      "[415395 rows x 2 columns]\n",
      "           uid   sid\n",
      "382      26554  3012\n",
      "383      26554  1681\n",
      "384      26554   201\n",
      "399      26554  3177\n",
      "401      26554  3289\n",
      "...        ...   ...\n",
      "5153229  26934   737\n",
      "5153233  26934   228\n",
      "5153236  26934   235\n",
      "5153240  26934  3962\n",
      "5153243  26934  1086\n",
      "\n",
      "[102295 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#데이터 셋 확인\n",
    "print(train_data)\n",
    "print(vad_data_tr)\n",
    "print(vad_data_te)\n",
    "# print(test_data_tr)\n",
    "# print(test_data_te)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    '''\n",
    "    Load Movielens dataset\n",
    "    '''\n",
    "    def __init__(self, path):\n",
    "\n",
    "        self.pro_dir = os.path.join(path, 'pro_sg')\n",
    "        assert os.path.exists(self.pro_dir), \"Preprocessed files do not exist. Run data.py\"\n",
    "\n",
    "        self.n_items = self.load_n_items()\n",
    "\n",
    "    def load_data(self, datatype='train'):\n",
    "        if datatype == 'train':\n",
    "            return self._load_train_data()\n",
    "        elif datatype == 'validation':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'test':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        elif datatype == 'check':\n",
    "            return self._load_tr_te_data(datatype)\n",
    "        else:\n",
    "            raise ValueError(\"datatype should be in [train, validation, test]\")\n",
    "\n",
    "    def load_n_items(self):\n",
    "        unique_sid = list()\n",
    "        with open(os.path.join(self.pro_dir, 'unique_sid.txt'), 'r') as f:\n",
    "            for line in f:\n",
    "                unique_sid.append(line.strip())\n",
    "        n_items = len(unique_sid)\n",
    "        return n_items\n",
    "\n",
    "    def _load_train_data(self):\n",
    "        path = os.path.join(self.pro_dir, 'train.csv')\n",
    "\n",
    "        tp = pd.read_csv(path)\n",
    "        n_users = tp['uid'].max() + 1\n",
    "\n",
    "        rows, cols = tp['uid'], tp['sid']\n",
    "        data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, self.n_items))\n",
    "        return data\n",
    "\n",
    "    def _load_tr_te_data(self, datatype='test'):\n",
    "        tr_path = os.path.join(self.pro_dir, '{}_tr.csv'.format(datatype))\n",
    "        te_path = os.path.join(self.pro_dir, '{}_te.csv'.format(datatype))\n",
    "\n",
    "        tp_tr = pd.read_csv(tr_path)\n",
    "        tp_te = pd.read_csv(te_path)\n",
    "\n",
    "        start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
    "        end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
    "\n",
    "        rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
    "        rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
    "\n",
    "        data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
    "                                    (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
    "                                    (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, self.n_items))\n",
    "        return data_tr, data_te"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Embedding with Word2Vec\n",
    "Word2vec를 활용하여 side information 사용하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Genre Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from urllib.request import urlretrieve, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", filename=\"GoogleNews-vectors-negative300.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 크기(shape) : (3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "print('모델의 크기(shape) :',word2vec_model.vectors.shape) # 모델의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>318</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>318</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2571</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2571</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2571</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre\n",
       "0   318     Crime\n",
       "1   318     Drama\n",
       "2  2571    Action\n",
       "3  2571    Sci-Fi\n",
       "4  2571  Thriller"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = pd.read_csv(\"/opt/ml/input/data/train/genres.tsv\", delimiter='\\t')\n",
    "gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gen_numerize(tp, show2id):\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre\n",
       "0   198     Crime\n",
       "1   198     Drama\n",
       "2    82    Action\n",
       "3    82    Sci-Fi\n",
       "4    82  Thriller"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen['item'] = gen_numerize(gen, show2id)\n",
    "gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      genre\n",
       "0     Drama\n",
       "1    Comedy\n",
       "2  Thriller\n",
       "3   Romance\n",
       "4    Action"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_emb = pd.DataFrame(gen['genre'].value_counts().index.values, columns=['genre'])\n",
    "gen_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "for x in gen_emb.genre:\n",
    "    if x == 'Sci-Fi':\n",
    "        emb_list.append(word2vec_model['science_fiction'])\n",
    "    elif x == 'Film-Noir':\n",
    "        emb_list.append(word2vec_model['Film_Noir'])\n",
    "    else:\n",
    "        emb_list.append(word2vec_model[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genre</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drama</th>\n",
       "      <td>-0.144531</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>-0.111816</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.277344</td>\n",
       "      <td>-0.127930</td>\n",
       "      <td>-0.320312</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222656</td>\n",
       "      <td>-0.100098</td>\n",
       "      <td>-0.589844</td>\n",
       "      <td>-0.184570</td>\n",
       "      <td>0.189453</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>-0.113281</td>\n",
       "      <td>-0.055908</td>\n",
       "      <td>0.052490</td>\n",
       "      <td>0.247070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comedy</th>\n",
       "      <td>-0.032959</td>\n",
       "      <td>-0.077637</td>\n",
       "      <td>-0.065918</td>\n",
       "      <td>0.291016</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.151367</td>\n",
       "      <td>0.273438</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>-0.054443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443359</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>-0.443359</td>\n",
       "      <td>-0.024048</td>\n",
       "      <td>-0.036621</td>\n",
       "      <td>0.253906</td>\n",
       "      <td>-0.046631</td>\n",
       "      <td>-0.045898</td>\n",
       "      <td>0.038818</td>\n",
       "      <td>0.074219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thriller</th>\n",
       "      <td>0.217773</td>\n",
       "      <td>-0.064941</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.314453</td>\n",
       "      <td>-0.047363</td>\n",
       "      <td>-0.192383</td>\n",
       "      <td>-0.390625</td>\n",
       "      <td>0.135742</td>\n",
       "      <td>0.168945</td>\n",
       "      <td>0.019165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058594</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.235352</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-0.034180</td>\n",
       "      <td>-0.396484</td>\n",
       "      <td>-0.028076</td>\n",
       "      <td>0.072266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Romance</th>\n",
       "      <td>0.041992</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.341797</td>\n",
       "      <td>0.119629</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>-0.057617</td>\n",
       "      <td>-0.179688</td>\n",
       "      <td>-0.090332</td>\n",
       "      <td>0.179688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116699</td>\n",
       "      <td>-0.078613</td>\n",
       "      <td>-0.322266</td>\n",
       "      <td>-0.013245</td>\n",
       "      <td>0.353516</td>\n",
       "      <td>-0.083008</td>\n",
       "      <td>-0.121094</td>\n",
       "      <td>0.022095</td>\n",
       "      <td>-0.035156</td>\n",
       "      <td>0.291016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Action</th>\n",
       "      <td>0.053955</td>\n",
       "      <td>-0.031250</td>\n",
       "      <td>0.242188</td>\n",
       "      <td>0.049316</td>\n",
       "      <td>0.023315</td>\n",
       "      <td>-0.080566</td>\n",
       "      <td>-0.059570</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>-0.310547</td>\n",
       "      <td>0.108887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009277</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>-0.242188</td>\n",
       "      <td>0.067383</td>\n",
       "      <td>0.024170</td>\n",
       "      <td>0.016968</td>\n",
       "      <td>0.049072</td>\n",
       "      <td>0.011475</td>\n",
       "      <td>-0.025513</td>\n",
       "      <td>-0.099121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5    \\\n",
       "genre                                                                  \n",
       "Drama    -0.144531 -0.055420  0.013855 -0.111816  0.187500  0.022095   \n",
       "Comedy   -0.032959 -0.077637 -0.065918  0.291016  0.041016  0.043213   \n",
       "Thriller  0.217773 -0.064941  0.187500  0.314453 -0.047363 -0.192383   \n",
       "Romance   0.041992 -0.075195 -0.341797  0.119629  0.185547 -0.005249   \n",
       "Action    0.053955 -0.031250  0.242188  0.049316  0.023315 -0.080566   \n",
       "\n",
       "               6         7         8         9    ...       290       291  \\\n",
       "genre                                             ...                       \n",
       "Drama    -0.277344 -0.127930 -0.320312  0.032227  ... -0.222656 -0.100098   \n",
       "Comedy    0.151367  0.273438  0.097656 -0.054443  ...  0.443359  0.078125   \n",
       "Thriller -0.390625  0.135742  0.168945  0.019165  ... -0.058594  0.003937   \n",
       "Romance  -0.057617 -0.179688 -0.090332  0.179688  ...  0.116699 -0.078613   \n",
       "Action   -0.059570  0.033203 -0.310547  0.108887  ...  0.009277 -0.189453   \n",
       "\n",
       "               292       293       294       295       296       297  \\\n",
       "genre                                                                  \n",
       "Drama    -0.589844 -0.184570  0.189453  0.195312 -0.113281 -0.055908   \n",
       "Comedy   -0.443359 -0.024048 -0.036621  0.253906 -0.046631 -0.045898   \n",
       "Thriller -0.267578  0.235352  0.271484 -0.000277 -0.034180 -0.396484   \n",
       "Romance  -0.322266 -0.013245  0.353516 -0.083008 -0.121094  0.022095   \n",
       "Action   -0.242188  0.067383  0.024170  0.016968  0.049072  0.011475   \n",
       "\n",
       "               298       299  \n",
       "genre                         \n",
       "Drama     0.052490  0.247070  \n",
       "Comedy    0.038818  0.074219  \n",
       "Thriller -0.028076  0.072266  \n",
       "Romance  -0.035156  0.291016  \n",
       "Action   -0.025513 -0.099121  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.concat([gen_emb, pd.DataFrame(emb_list)], axis=1)\n",
    "a = x.set_index('genre', drop=True)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen2emb = dict((x, a.loc[x].values) for (i, x) in enumerate(a.index))\n",
    "gen['emb'] = gen['genre'].apply(lambda x: gen2emb[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>genre</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>198</td>\n",
       "      <td>Crime</td>\n",
       "      <td>[0.028076171875, 0.0048828125, -0.0966796875, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[-0.14453125, -0.055419921875, 0.0138549804687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Action</td>\n",
       "      <td>[0.053955078125, -0.03125, 0.2421875, 0.049316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82</td>\n",
       "      <td>Sci-Fi</td>\n",
       "      <td>[0.1298828125, -0.1259765625, 0.1796875, 0.251...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>[0.2177734375, -0.06494140625, 0.1875, 0.31445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>260</td>\n",
       "      <td>Action</td>\n",
       "      <td>[0.053955078125, -0.03125, 0.2421875, 0.049316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>260</td>\n",
       "      <td>Crime</td>\n",
       "      <td>[0.028076171875, 0.0048828125, -0.0966796875, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>260</td>\n",
       "      <td>Drama</td>\n",
       "      <td>[-0.14453125, -0.055419921875, 0.0138549804687...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>260</td>\n",
       "      <td>Thriller</td>\n",
       "      <td>[0.2177734375, -0.06494140625, 0.1875, 0.31445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>264</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>[-0.032958984375, -0.07763671875, -0.065917968...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item     genre                                                emb\n",
       "0   198     Crime  [0.028076171875, 0.0048828125, -0.0966796875, ...\n",
       "1   198     Drama  [-0.14453125, -0.055419921875, 0.0138549804687...\n",
       "2    82    Action  [0.053955078125, -0.03125, 0.2421875, 0.049316...\n",
       "3    82    Sci-Fi  [0.1298828125, -0.1259765625, 0.1796875, 0.251...\n",
       "4    82  Thriller  [0.2177734375, -0.06494140625, 0.1875, 0.31445...\n",
       "5   260    Action  [0.053955078125, -0.03125, 0.2421875, 0.049316...\n",
       "6   260     Crime  [0.028076171875, 0.0048828125, -0.0966796875, ...\n",
       "7   260     Drama  [-0.14453125, -0.055419921875, 0.0138549804687...\n",
       "8   260  Thriller  [0.2177734375, -0.06494140625, 0.1875, 0.31445...\n",
       "9   264    Comedy  [-0.032958984375, -0.07763671875, -0.065917968..."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 평균을 취하지 않고 각 아이템 별로 장르를 하나씩만 남기면 어떻게 될까요?\n",
    "# gen = gen.drop_duplicates(subset=['item'])\n",
    "# gen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total = []\n",
    "def item_genre_emb_mean(i):\n",
    "    total.append(np.mean(gen[gen['item'] == i].emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016510</td>\n",
       "      <td>-0.062561</td>\n",
       "      <td>0.094589</td>\n",
       "      <td>0.042084</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.026520</td>\n",
       "      <td>-0.068420</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>-0.155029</td>\n",
       "      <td>-0.025269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.132629</td>\n",
       "      <td>-0.041382</td>\n",
       "      <td>-0.305176</td>\n",
       "      <td>-0.006836</td>\n",
       "      <td>0.197449</td>\n",
       "      <td>0.032135</td>\n",
       "      <td>-0.027195</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.043518</td>\n",
       "      <td>0.061951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081635</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>0.068909</td>\n",
       "      <td>0.082504</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>-0.055420</td>\n",
       "      <td>-0.080139</td>\n",
       "      <td>0.152634</td>\n",
       "      <td>-0.122925</td>\n",
       "      <td>0.063751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075012</td>\n",
       "      <td>-0.030418</td>\n",
       "      <td>-0.158997</td>\n",
       "      <td>0.145264</td>\n",
       "      <td>0.174988</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.155701</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.040222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.103394</td>\n",
       "      <td>0.007202</td>\n",
       "      <td>-0.018585</td>\n",
       "      <td>0.072998</td>\n",
       "      <td>0.104065</td>\n",
       "      <td>0.027039</td>\n",
       "      <td>-0.024414</td>\n",
       "      <td>0.055664</td>\n",
       "      <td>-0.175781</td>\n",
       "      <td>-0.042969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228516</td>\n",
       "      <td>-0.050545</td>\n",
       "      <td>-0.402344</td>\n",
       "      <td>-0.223145</td>\n",
       "      <td>0.194824</td>\n",
       "      <td>0.049561</td>\n",
       "      <td>-0.035889</td>\n",
       "      <td>-0.060669</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.212891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.081909</td>\n",
       "      <td>-0.063721</td>\n",
       "      <td>0.080933</td>\n",
       "      <td>0.164795</td>\n",
       "      <td>0.060608</td>\n",
       "      <td>0.061646</td>\n",
       "      <td>0.236816</td>\n",
       "      <td>0.079346</td>\n",
       "      <td>-0.057861</td>\n",
       "      <td>-0.184082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273438</td>\n",
       "      <td>-0.071785</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.039551</td>\n",
       "      <td>0.143799</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>-0.082275</td>\n",
       "      <td>0.070801</td>\n",
       "      <td>-0.091904</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054626</td>\n",
       "      <td>-0.195557</td>\n",
       "      <td>-0.029028</td>\n",
       "      <td>-0.056213</td>\n",
       "      <td>0.179199</td>\n",
       "      <td>-0.013428</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.259277</td>\n",
       "      <td>-0.032227</td>\n",
       "      <td>-0.060394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173462</td>\n",
       "      <td>0.118408</td>\n",
       "      <td>-0.044434</td>\n",
       "      <td>-0.103149</td>\n",
       "      <td>0.124237</td>\n",
       "      <td>-0.050903</td>\n",
       "      <td>-0.112061</td>\n",
       "      <td>-0.040283</td>\n",
       "      <td>-0.031189</td>\n",
       "      <td>0.062378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.016510 -0.062561  0.094589  0.042084  0.043182  0.026520 -0.068420   \n",
       "1  0.081635 -0.032227  0.068909  0.082504  0.009857 -0.055420 -0.080139   \n",
       "2 -0.103394  0.007202 -0.018585  0.072998  0.104065  0.027039 -0.024414   \n",
       "3 -0.081909 -0.063721  0.080933  0.164795  0.060608  0.061646  0.236816   \n",
       "4  0.054626 -0.195557 -0.029028 -0.056213  0.179199 -0.013428  0.020264   \n",
       "\n",
       "        7         8         9    ...       290       291       292       293  \\\n",
       "0  0.137451 -0.155029 -0.025269  ... -0.132629 -0.041382 -0.305176 -0.006836   \n",
       "1  0.152634 -0.122925  0.063751  ... -0.075012 -0.030418 -0.158997  0.145264   \n",
       "2  0.055664 -0.175781 -0.042969  ... -0.228516 -0.050545 -0.402344 -0.223145   \n",
       "3  0.079346 -0.057861 -0.184082  ... -0.273438 -0.071785 -0.208496 -0.039551   \n",
       "4  0.259277 -0.032227 -0.060394  ...  0.173462  0.118408 -0.044434 -0.103149   \n",
       "\n",
       "        294       295       296       297       298       299  \n",
       "0  0.197449  0.032135 -0.027195  0.064209  0.043518  0.061951  \n",
       "1  0.174988  0.007255  0.039490 -0.155701  0.000488 -0.040222  \n",
       "2  0.194824  0.049561 -0.035889 -0.060669  0.027115  0.212891  \n",
       "3  0.143799  0.001709 -0.082275  0.070801 -0.091904  0.053711  \n",
       "4  0.124237 -0.050903 -0.112061 -0.040283 -0.031189  0.062378  \n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_genre_emb_idx = pd.DataFrame(list(i for i in range(0, max(gen.item)+1)), columns=['item'])\n",
    "item_genre_emb_idx.item.apply(lambda x: item_genre_emb_mean(x))\n",
    "item_genre_emb = pd.DataFrame(total)\n",
    "item_genre_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 300)\n"
     ]
    }
   ],
   "source": [
    "print(item_genre_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6807)\n"
     ]
    }
   ],
   "source": [
    "item_genre_emb = item_genre_emb.T\n",
    "print(item_genre_emb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Title Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import sister\n",
    "sentence_embedding = sister.MeanEmbedding(lang=\"en\")\n",
    "# # bert_embedding = sister.BertEmbedding(lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# sbert_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "title = pd.read_csv(\"/opt/ml/input/data/train/titles.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        item                                              title\n",
      "0        318                   Shawshank Redemption, The (1994)\n",
      "1       2571                                 Matrix, The (1999)\n",
      "2       2959                                  Fight Club (1999)\n",
      "3        296                                Pulp Fiction (1994)\n",
      "4        356                                Forrest Gump (1994)\n",
      "...      ...                                                ...\n",
      "6802   73106  American Pie Presents: The Book of Love (Ameri...\n",
      "6803  109850                              Need for Speed (2014)\n",
      "6804    8605                                      Taxi 3 (2003)\n",
      "6805    3689                    Porky's II: The Next Day (1983)\n",
      "6806    8130                         Girl Next Door, The (1999)\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def title_numerize(tp, show2id):\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return sid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "0      198                   Shawshank Redemption, The (1994)\n",
      "1       82                                 Matrix, The (1999)\n",
      "2      260                                  Fight Club (1999)\n",
      "3      264                                Pulp Fiction (1994)\n",
      "4      265                                Forrest Gump (1994)\n",
      "...    ...                                                ...\n",
      "6802  3396  American Pie Presents: The Book of Love (Ameri...\n",
      "6803  6763                              Need for Speed (2014)\n",
      "6804  5046                                      Taxi 3 (2003)\n",
      "6805  5508                    Porky's II: The Next Day (1983)\n",
      "6806  5531                         Girl Next Door, The (1999)\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "title['item'] = title_numerize(title, show2id)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /opt/ml/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /opt/ml/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_title = []\n",
    "\n",
    "for item, text in title.values:\n",
    "    new_text = ''\n",
    "    if re.search(r'\\([0-9]{4}\\)', text[-6:]):\n",
    "        new_text = text[:-6]\n",
    "    else:\n",
    "        new_text = text[:text.rfind('(')]\n",
    "\n",
    "    # new_text = re.compile(\"\\W+\").sub(\" \", new_text)\n",
    "    # filtered_tokens = [token for token in word_tokenize(new_text) if token.lower() not in stops]\n",
    "    # new_title.append([item, ' '.join(filtered_tokens)])\n",
    "    new_title.append([item, new_text])\n",
    "\n",
    "new_title_df = pd.DataFrame(new_title, columns = ['item', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_title_dict = {}\n",
    "for item, title in new_title_df.values:\n",
    "    emb_title_dict[item] = sentence_embedding(title)\n",
    "    # emb_title_dict[item] = bert_embedding(title)\n",
    "    # emb_title_dict[item] = sbert_model.encode(title, convert_to_tensor=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "0      198  [0.09887906, 0.0020635799, 0.1023672, -0.16108...\n",
      "1       82  [0.20249972, -0.116814286, 0.038145967, -0.091...\n",
      "2      260  [0.15916233, -0.013797939, -0.06912231, -0.066...\n",
      "3      264  [-0.042834193, -0.026048277, 0.048151728, -0.0...\n",
      "4      265  [0.18826124, 0.06215177, 0.04759519, -0.018898...\n",
      "...    ...                                                ...\n",
      "6802  3396  [0.05855634, -0.07672383, 0.056784138, -0.1529...\n",
      "6803  6763  [0.0046750098, 0.07040122, 0.19956714, -0.1759...\n",
      "6804  5046  [0.1844265, -0.07056019, -0.04190911, 0.080615...\n",
      "6805  5508  [0.24258502, -0.008186941, 0.0679334, -0.11687...\n",
      "6806  5531  [0.16492175, -0.087725565, -0.018908694, -0.05...\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "emb_title_df = pd.DataFrame(list(emb_title_dict.items()), columns=['item', 'title'])\n",
    "print(emb_title_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      item                                              title\n",
      "3492     0  [0.060705453, -0.17232932, 0.07193938, -0.0160...\n",
      "4567     1  [-0.048258446, 0.29175144, 0.024340292, -0.248...\n",
      "737      2  [0.16013275, 0.01752352, 0.09133945, -0.174270...\n",
      "731      3  [0.14779359, -0.1571878, 0.11643047, -0.173786...\n",
      "574      4  [0.22029698, 0.039300818, 0.08298388, -0.19853...\n",
      "...    ...                                                ...\n",
      "6289  6802  [0.09505787, 0.00903964, -0.09849598, -0.06931...\n",
      "6772  6803  [0.13124995, -0.05749653, 0.0032128196, -0.057...\n",
      "6760  6804  [0.20940778, -0.06612897, 0.14894299, -0.14761...\n",
      "6516  6805  [0.11617702, -0.23807585, -0.010553868, -0.435...\n",
      "6762  6806  [0.05673375, -0.018610748, -0.15926576, -0.130...\n",
      "\n",
      "[6807 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "emb_title_df2 = emb_title_df.sort_values(by=['item'])\n",
    "print(emb_title_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6    \\\n",
      "0     0.060705 -0.172329  0.071939 -0.016023  0.019821  0.172676 -0.098369   \n",
      "1    -0.048258  0.291751  0.024340 -0.248048  0.019833 -0.042012  0.269257   \n",
      "2     0.160133  0.017524  0.091339 -0.174270  0.144272  0.108190 -0.209995   \n",
      "3     0.147794 -0.157188  0.116430 -0.173786  0.058035 -0.050341 -0.161892   \n",
      "4     0.220297  0.039301  0.082984 -0.198539  0.107078  0.112428 -0.267611   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "6802  0.095058  0.009040 -0.098496 -0.069320  0.111002 -0.080911 -0.205108   \n",
      "6803  0.131250 -0.057497  0.003213 -0.057943  0.102392  0.084665 -0.090912   \n",
      "6804  0.209408 -0.066129  0.148943 -0.147610  0.045850  0.033999 -0.186204   \n",
      "6805  0.116177 -0.238076 -0.010554 -0.435312  0.071545 -0.130239 -0.190550   \n",
      "6806  0.056734 -0.018611 -0.159266 -0.130187  0.004537  0.113627 -0.185436   \n",
      "\n",
      "           7         8         9    ...       290       291       292  \\\n",
      "0    -0.015639 -0.050379 -0.051945  ... -0.125337  0.011902  0.045414   \n",
      "1    -0.177424  0.416401  0.327475  ...  0.162379  0.000952  0.293767   \n",
      "2     0.058064  0.010407  0.159249  ... -0.077829 -0.099417 -0.094958   \n",
      "3    -0.013702  0.000790 -0.029409  ...  0.046678 -0.103129  0.011876   \n",
      "4    -0.094678  0.091287  0.039513  ...  0.021672  0.059619 -0.070881   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "6802 -0.040041  0.324735 -0.052425  ...  0.069953  0.190423 -0.086988   \n",
      "6803 -0.063237 -0.035511  0.075327  ... -0.066396  0.044142 -0.014393   \n",
      "6804 -0.088744  0.131297 -0.030582  ...  0.077845 -0.098284  0.021381   \n",
      "6805 -0.153314  0.235201 -0.091391  ...  0.006570 -0.277720 -0.003019   \n",
      "6806 -0.101376  0.000113  0.069646  ...  0.075978 -0.108760  0.108718   \n",
      "\n",
      "           293       294       295       296       297       298       299  \n",
      "0    -0.069140  0.058895  0.013648 -0.098602  0.001309 -0.170907 -0.064850  \n",
      "1    -0.234219  0.011926  0.178708 -0.051436  0.067603  0.019331  0.097408  \n",
      "2    -0.117981 -0.024513 -0.030071  0.088674  0.111612  0.068765 -0.044972  \n",
      "3    -0.138554 -0.025504  0.148756  0.031759  0.033528  0.088699  0.012696  \n",
      "4    -0.010764  0.089532  0.061731 -0.114149  0.035243 -0.023510  0.031119  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "6802 -0.140079  0.012981 -0.094743 -0.277142 -0.065319 -0.396279  0.121280  \n",
      "6803 -0.034365  0.011282  0.091712 -0.028294  0.010583  0.021816 -0.055442  \n",
      "6804 -0.179527  0.022492  0.048297 -0.070032  0.033208  0.058636 -0.032276  \n",
      "6805  0.165771  0.128352 -0.066401  0.071748  0.052234  0.060959 -0.125069  \n",
      "6806  0.011436 -0.023316  0.036824 -0.096715  0.024286 -0.007445 -0.067098  \n",
      "\n",
      "[6807 rows x 300 columns]\n"
     ]
    }
   ],
   "source": [
    "total_emb_title = []\n",
    "\n",
    "for text in emb_title_df2['title'].values:\n",
    "    text = text.tolist()\n",
    "    total_emb_title.append(text)\n",
    "\n",
    "item_title_emb = pd.DataFrame(total_emb_title)\n",
    "print(item_title_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6807, 300)\n"
     ]
    }
   ],
   "source": [
    "print(item_title_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 6807)\n"
     ]
    }
   ],
   "source": [
    "item_title_emb = item_title_emb.T\n",
    "print(item_title_emb.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model\n",
    "## Multi-VAE and Multi-DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# 데이터 로드\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiDAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-DAE.\n",
    "\n",
    "    Multi-DAE : Denoising Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiDAE, self).__init__()\n",
    "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
    "        self.item_title = torch.Tensor(item_title_emb.values) ##### 추가\n",
    "\n",
    "        self.p_dims = p_dims\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        self.dims = self.q_dims + self.p_dims[1:]\n",
    "        self.layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.dims[:-1], self.dims[1:])])\n",
    "        # 잡음 추가\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        #print('input.shape: ', input.shape)\n",
    "        h = F.normalize(input)\n",
    "        # h = self.drop(h)\n",
    "        # h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
    "\n",
    "        # item_genre_dropped = self.drop(self.item_genre)\n",
    "        # item_title_dropped = self.drop(self.item_title)\n",
    "\n",
    "        h = torch.cat((self.item_genre.to(device), self.item_title.to(device), h), 0) ###추가\n",
    "        h = self.drop(h)\n",
    "        #print('합친 h.shape: ', h.shape)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.layers) - 1:\n",
    "                h = F.tanh(h) #reluX\n",
    "\n",
    "        # reconstructed_genre, reconstructed_h = h.split([self.item_genre.shape[0], input.shape[0]], 0) ##추가\n",
    "        reconstructed_genre, reconstructed_title, reconstructed_h = h.split([self.item_genre.shape[0], self.item_title.shape[0], input.shape[0]], 0) ##추가\n",
    "        return reconstructed_genre, reconstructed_title, reconstructed_h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MultiVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Container module for Multi-VAE.\n",
    "\n",
    "    Multi-VAE : Variational Autoencoder with Multinomial Likelihood\n",
    "    See Variational Autoencoders for Collaborative Filtering\n",
    "    https://arxiv.org/abs/1802.05814\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_dims, q_dims=None, dropout=0.5):\n",
    "        super(MultiVAE, self).__init__()\n",
    "        self.p_dims = p_dims\n",
    "        self.item_genre = torch.Tensor(item_genre_emb.values) ##### 추가\n",
    "        self.item_title = torch.Tensor(item_title_emb.values) ##### 추가\n",
    "        if q_dims:\n",
    "            assert q_dims[0] == p_dims[-1], \"In and Out dimensions must equal to each other\"\n",
    "            assert q_dims[-1] == p_dims[0], \"Latent dimension for p- and q- network mismatches.\"\n",
    "            self.q_dims = q_dims\n",
    "        else:\n",
    "            self.q_dims = p_dims[::-1]\n",
    "\n",
    "        # Last dimension of q- network is for mean and variance\n",
    "        temp_q_dims = self.q_dims[:-1] + [self.q_dims[-1] * 2]\n",
    "        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(temp_q_dims[:-1], temp_q_dims[1:])])\n",
    "        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) for\n",
    "            d_in, d_out in zip(self.p_dims[:-1], self.p_dims[1:])])\n",
    "\n",
    "        self.h_temp = None\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_gen, recon_tit, recon_batch = self.decode(z)\n",
    "        return recon_gen, recon_tit, recon_batch, mu, logvar\n",
    "\n",
    "    def encode(self, input):\n",
    "        h = F.normalize(input)\n",
    "        # h = self.drop(h)\n",
    "\n",
    "        # item_genre_dropped = self.drop(self.item_genre)\n",
    "        # item_title_dropped = self.drop(self.item_title)\n",
    "\n",
    "        # h = torch.cat((self.item_genre.to(device), h), 0) ###추가\n",
    "        h = torch.cat((self.item_genre.to(device), self.item_title.to(device), h), 0) ###추가\n",
    "        h = self.drop(h)\n",
    "        for i, layer in enumerate(self.q_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.q_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "            else:\n",
    "                mu = h[:, :self.q_dims[-1]]\n",
    "                logvar = h[:, self.q_dims[-1]:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = z\n",
    "        for i, layer in enumerate(self.p_layers):\n",
    "            h = layer(h)\n",
    "            if i != len(self.p_layers) - 1:\n",
    "                h = F.tanh(h)\n",
    "\n",
    "        #reconstructed_genre, reconstructed_h = h.split([self.item_genre.shape[0], self.input.shape[0]], 0) ##추가\n",
    "        reconstructed_genre, reconstructed_title, reconstructed_h = h.split([self.item_genre.shape[0], self.item_title.shape[0], self.input.shape[0]], 0) ##추가\n",
    "        return reconstructed_genre, reconstructed_title, reconstructed_h\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.q_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)\n",
    "\n",
    "        for layer in self.p_layers:\n",
    "            # Xavier Initialization for weights\n",
    "            size = layer.weight.size()\n",
    "            fan_out = size[0]\n",
    "            fan_in = size[1]\n",
    "            std = np.sqrt(2.0/(fan_in + fan_out))\n",
    "            layer.weight.data.normal_(0.0, std)\n",
    "\n",
    "            # Normal Initialization for Biases\n",
    "            layer.bias.data.normal_(0.0, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function_vae(recon_x, x, mu, logvar, anneal=1.0):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    return BCE + anneal * KLD\n",
    "\n",
    "def loss_function_dae(recon_x, x):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    return BCE\n",
    "\n",
    "def loss_function_vae_genre(recon_genre, recon_title, recon_x, x, mu, logvar, anneal=1.0):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    mseloss = nn.MSELoss()\n",
    "    mseloss2 = nn.MSELoss()\n",
    "\n",
    "    RMSE = mseloss(recon_genre, torch.Tensor(item_genre_emb.values).to(device))\n",
    "    RMSE2 = mseloss2(recon_title, torch.Tensor(item_title_emb.values).to(device))\n",
    "\n",
    "    return BCE + anneal * KLD + RMSE + RMSE2\n",
    "\n",
    "def loss_function_dae_genre(recon_genre, recon_title, recon_x, x):\n",
    "    BCE = -torch.mean(torch.sum(F.log_softmax(recon_x, 1) * x, -1))\n",
    "\n",
    "    mseloss = nn.MSELoss()\n",
    "    mseloss2 = nn.MSELoss()\n",
    "\n",
    "    RMSE = mseloss(recon_genre, torch.Tensor(item_genre_emb.values).to(device))\n",
    "    RMSE2 = mseloss2(recon_title, torch.Tensor(item_title_emb.values).to(device))\n",
    "\n",
    "    return BCE + RMSE + RMSE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sparse2torch_sparse(data):\n",
    "    \"\"\"\n",
    "    Convert scipy sparse matrix to torch sparse tensor with L2 Normalization\n",
    "    This is much faster than naive use of torch.FloatTensor(data.toarray())\n",
    "    https://discuss.pytorch.org/t/sparse-tensor-use-cases/22047/2\n",
    "    \"\"\"\n",
    "    samples = data.shape[0]\n",
    "    features = data.shape[1]\n",
    "    coo_data = data.tocoo()\n",
    "    indices = torch.LongTensor([coo_data.row, coo_data.col])\n",
    "    row_norms_inv = 1 / np.sqrt(data.sum(1))\n",
    "    row2val = {i : row_norms_inv[i].item() for i in range(samples)}\n",
    "    values = np.array([row2val[r] for r in coo_data.row])\n",
    "    t = torch.sparse.FloatTensor(indices, torch.from_numpy(values).float(), [samples, features])\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_sparse2tensor(data):\n",
    "    return torch.FloatTensor(data.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def NDCG_binary_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    '''\n",
    "    Normalized Discounted Cumulative Gain@k for binary relevance\n",
    "    ASSUMPTIONS: all the 0's in heldout_data indicate 0 relevance\n",
    "    '''\n",
    "    batch_users = X_pred.shape[0]\n",
    "    idx_topk_part = bn.argpartition(-X_pred, k, axis=1)\n",
    "    topk_part = X_pred[np.arange(batch_users)[:, np.newaxis],\n",
    "                       idx_topk_part[:, :k]]\n",
    "    idx_part = np.argsort(-topk_part, axis=1)\n",
    "\n",
    "    idx_topk = idx_topk_part[np.arange(batch_users)[:, np.newaxis], idx_part]\n",
    "\n",
    "    tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "\n",
    "    DCG = (heldout_batch[np.arange(batch_users)[:, np.newaxis],\n",
    "                         idx_topk].toarray() * tp).sum(axis=1)\n",
    "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
    "                     for n in heldout_batch.getnnz(axis=1)])\n",
    "    return DCG / IDCG\n",
    "\n",
    "\n",
    "def Recall_at_k_batch(X_pred, heldout_batch, k=100):\n",
    "    batch_users = X_pred.shape[0]\n",
    "\n",
    "    X_pred = X_pred\n",
    "    idx = bn.argpartition(-X_pred, k, axis=1)\n",
    "\n",
    "    X_pred_binary = np.zeros_like(X_pred, dtype=bool)\n",
    "    X_pred_binary[np.arange(batch_users)[:, np.newaxis], idx[:, :k]] = True\n",
    "\n",
    "    X_true_binary = (heldout_batch > 0).toarray()\n",
    "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
    "        np.float32)\n",
    "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
    "    return recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, epoch, is_VAE = False):\n",
    "    # Turn on training mode\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    global update_count\n",
    "\n",
    "    np.random.shuffle(idxlist)\n",
    "\n",
    "    for batch_idx, start_idx in enumerate(range(0, N, args.batch_size)):\n",
    "        end_idx = min(start_idx + args.batch_size, N)\n",
    "        data = train_data[idxlist[start_idx:end_idx]]\n",
    "        data = naive_sparse2tensor(data).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if is_VAE:\n",
    "            if args.total_anneal_steps > 0:\n",
    "                anneal = min(args.anneal_cap,\n",
    "                            1. * update_count / args.total_anneal_steps)\n",
    "            else:\n",
    "                anneal = args.anneal_cap\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #recon_batch, mu, logvar = model(data)\n",
    "            recon_genre, recon_title, recon_batch, mu, logvar = model(data)\n",
    "\n",
    "            #loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "            loss = criterion(recon_batch, data, mu, logvar, anneal)\n",
    "        else:\n",
    "            #recon_batch = model(data)\n",
    "            recon_genre, recon_title, recon_batch = model(data)\n",
    "            # loss = criterion(recon_genre, recon_title, recon_batch, data)\n",
    "            loss = criterion(recon_batch, data)\n",
    "\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        update_count += 1\n",
    "\n",
    "        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:4d}/{:4d} batches | ms/batch {:4.2f} | '\n",
    "                    'loss {:4.2f}'.format(\n",
    "                        epoch, batch_idx, len(range(0, N, args.batch_size)),\n",
    "                        elapsed * 1000 / args.log_interval,\n",
    "                        train_loss / args.log_interval))\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "    train_loss /= len(range(0, N, args.batch_size))\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_tr, data_te, is_VAE=False):\n",
    "    # Turn on evaluation mode\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    global update_count\n",
    "    e_idxlist = list(range(data_tr.shape[0]))\n",
    "    e_N = data_tr.shape[0]\n",
    "    n100_list = []\n",
    "    r10_list= []\n",
    "    r20_list = []\n",
    "    r50_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, e_N, args.batch_size):\n",
    "            end_idx = min(start_idx + args.batch_size, N)\n",
    "            data = data_tr[e_idxlist[start_idx:end_idx]]\n",
    "            heldout_data = data_te[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "            data_tensor = naive_sparse2tensor(data).to(device)\n",
    "            if is_VAE :\n",
    "\n",
    "              if args.total_anneal_steps > 0:\n",
    "                  anneal = min(args.anneal_cap,\n",
    "                                1. * update_count / args.total_anneal_steps)\n",
    "              else:\n",
    "                  anneal = args.anneal_cap\n",
    "\n",
    "              #recon_batch, mu, logvar = model(data_tensor)\n",
    "              #recon_genre, recon_batch, mu, logvar = model(data_tensor)\n",
    "              recon_genre, recon_title, recon_batch, mu, logvar = model(data_tensor)\n",
    "\n",
    "              loss = criterion(recon_batch, data_tensor, mu, logvar, anneal)\n",
    "              # loss = criterion(recon_genre, recon_batch, data_tensor, mu, logvar, anneal)\n",
    "              # loss = criterion(recon_genre, recon_title, recon_batch, data_tensor, mu, logvar, anneal)\n",
    "\n",
    "            else :\n",
    "              #recon_batch = model(data_tensor)\n",
    "              #recon_genre, recon_batch = model(data_tensor)\n",
    "              recon_genre, recon_title, recon_batch = model(data_tensor)\n",
    "\n",
    "              loss = criterion(recon_batch, data_tensor)\n",
    "              #loss = criterion(recon_genre, recon_batch, data_tensor)\n",
    "              #loss = criterion(recon_genre, recon_title, recon_batch, data_tensor)\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Exclude examples from training set\n",
    "            recon_batch = recon_batch.cpu().numpy()\n",
    "            recon_genre = recon_genre.cpu().numpy()\n",
    "            recon_title = recon_title.cpu().numpy()\n",
    "            recon_batch[data.nonzero()] = -np.inf\n",
    "\n",
    "            n100 = NDCG_binary_at_k_batch(recon_batch, heldout_data, 100)\n",
    "            r10 = Recall_at_k_batch(recon_batch, heldout_data, 10)\n",
    "            r20 = Recall_at_k_batch(recon_batch, heldout_data, 20)\n",
    "            r50 = Recall_at_k_batch(recon_batch, heldout_data, 50)\n",
    "\n",
    "            # r10 = Recall_at_k_batch(recon_genre, recon_title, recon_batch, heldout_data, 10)\n",
    "            # r20 = Recall_at_k_batch(recon_genre, recon_title, recon_batch, heldout_data, 20)\n",
    "            # r50 = Recall_at_k_batch(recon_genre, recon_title, recon_batch, heldout_data, 50)\n",
    "\n",
    "            n100_list.append(n100)\n",
    "            r20_list.append(r20)\n",
    "            r10_list.append(r10)\n",
    "            r50_list.append(r50)\n",
    "\n",
    "    total_loss /= len(range(0, e_N, args.batch_size))\n",
    "    n100_list = np.concatenate(n100_list)\n",
    "    r20_list = np.concatenate(r20_list)\n",
    "    r10_list = np.concatenate(r10_list)\n",
    "    r50_list = np.concatenate(r50_list)\n",
    "\n",
    "    return total_loss, np.mean(n100_list), np.mean(r10_list), np.mean(r20_list), np.mean(r50_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Multi-DAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"movie_recommendation\", entity=\"glancyes\")\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "check_data_tr, check_data_te = loader.load_data('check')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "#p_dims = [200, 600, 1600, 3200, n_items]\n",
    "p_dims = [200, 3000, n_items]\n",
    "model_dae = MultiDAE(p_dims).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "optimizer = adabound.AdaBound(model_dae.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#https://github.com/Luolc/AdaBound\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "#criterion = loss_function_dae\n",
    "criterion = loss_function_dae\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0\n",
    "\n",
    "############batch 1600\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "r10_fin_list = []\n",
    "new_epochs = args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1933: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3.06s | valid loss 968.78 | n100 0.344 | r10 0.279 | r20 0.256 | r50 0.311\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 3.07s | valid loss 946.22 | n100 0.385 | r10 0.320 | r20 0.293 | r50 0.351\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 3.05s | valid loss 936.55 | n100 0.403 | r10 0.330 | r20 0.305 | r50 0.370\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 3.05s | valid loss 930.64 | n100 0.416 | r10 0.350 | r20 0.322 | r50 0.381\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 3.04s | valid loss 925.17 | n100 0.424 | r10 0.357 | r20 0.329 | r50 0.388\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 3.08s | valid loss 921.44 | n100 0.425 | r10 0.353 | r20 0.329 | r50 0.396\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 3.14s | valid loss 917.79 | n100 0.432 | r10 0.360 | r20 0.333 | r50 0.398\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 3.07s | valid loss 915.18 | n100 0.438 | r10 0.374 | r20 0.342 | r50 0.401\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 3.12s | valid loss 913.29 | n100 0.424 | r10 0.332 | r20 0.319 | r50 0.396\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 3.11s | valid loss 909.76 | n100 0.439 | r10 0.367 | r20 0.340 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 3.08s | valid loss 908.04 | n100 0.443 | r10 0.375 | r20 0.346 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 3.17s | valid loss 906.43 | n100 0.441 | r10 0.371 | r20 0.343 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 3.13s | valid loss 904.63 | n100 0.433 | r10 0.349 | r20 0.330 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 3.01s | valid loss 904.48 | n100 0.443 | r10 0.377 | r20 0.347 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 3.05s | valid loss 902.32 | n100 0.433 | r10 0.350 | r20 0.329 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 3.06s | valid loss 900.05 | n100 0.437 | r10 0.360 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 3.07s | valid loss 898.81 | n100 0.438 | r10 0.363 | r20 0.338 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 3.11s | valid loss 897.74 | n100 0.439 | r10 0.365 | r20 0.338 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 3.11s | valid loss 903.08 | n100 0.442 | r10 0.376 | r20 0.344 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 3.14s | valid loss 895.91 | n100 0.439 | r10 0.362 | r20 0.339 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 3.16s | valid loss 897.25 | n100 0.429 | r10 0.342 | r20 0.322 | r50 0.399\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 3.20s | valid loss 894.00 | n100 0.442 | r10 0.365 | r20 0.342 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 3.17s | valid loss 893.78 | n100 0.441 | r10 0.362 | r20 0.338 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 3.09s | valid loss 892.62 | n100 0.442 | r10 0.367 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 3.07s | valid loss 892.07 | n100 0.439 | r10 0.362 | r20 0.338 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 3.18s | valid loss 891.75 | n100 0.441 | r10 0.361 | r20 0.336 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 3.17s | valid loss 890.67 | n100 0.444 | r10 0.370 | r20 0.343 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 3.18s | valid loss 893.86 | n100 0.424 | r10 0.331 | r20 0.316 | r50 0.395\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 3.03s | valid loss 889.73 | n100 0.443 | r10 0.363 | r20 0.339 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 3.17s | valid loss 889.52 | n100 0.441 | r10 0.363 | r20 0.339 | r50 0.407\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 3.12s | valid loss 888.59 | n100 0.445 | r10 0.367 | r20 0.342 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 3.14s | valid loss 888.68 | n100 0.441 | r10 0.368 | r20 0.340 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 3.18s | valid loss 888.50 | n100 0.441 | r10 0.361 | r20 0.340 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 3.11s | valid loss 887.81 | n100 0.446 | r10 0.373 | r20 0.344 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 3.21s | valid loss 887.23 | n100 0.443 | r10 0.368 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 3.15s | valid loss 887.26 | n100 0.447 | r10 0.376 | r20 0.347 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 3.10s | valid loss 887.01 | n100 0.441 | r10 0.362 | r20 0.337 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 3.15s | valid loss 886.32 | n100 0.442 | r10 0.365 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 3.08s | valid loss 886.92 | n100 0.444 | r10 0.370 | r20 0.342 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 3.09s | valid loss 884.94 | n100 0.445 | r10 0.371 | r20 0.343 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 3.17s | valid loss 885.90 | n100 0.444 | r10 0.369 | r20 0.345 | r50 0.411\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 3.17s | valid loss 885.25 | n100 0.439 | r10 0.358 | r20 0.335 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 3.06s | valid loss 884.24 | n100 0.443 | r10 0.363 | r20 0.339 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 3.11s | valid loss 883.78 | n100 0.445 | r10 0.367 | r20 0.343 | r50 0.410\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 3.23s | valid loss 884.84 | n100 0.439 | r10 0.361 | r20 0.335 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 3.22s | valid loss 883.74 | n100 0.444 | r10 0.369 | r20 0.341 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 3.13s | valid loss 887.05 | n100 0.445 | r10 0.372 | r20 0.344 | r50 0.408\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 3.14s | valid loss 883.50 | n100 0.446 | r10 0.373 | r20 0.345 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 3.12s | valid loss 883.41 | n100 0.444 | r10 0.367 | r20 0.341 | r50 0.409\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 3.07s | valid loss 882.91 | n100 0.449 | r10 0.377 | r20 0.348 | r50 0.413\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, new_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model_dae, criterion, optimizer, epoch, is_VAE=False)\n",
    "    val_loss, n100, r10, r20, r50 = evaluate(model_dae, criterion, vad_data_tr, vad_data_te, is_VAE=False)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    r10_fin_list.append(r10)\n",
    "\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, val_loss,\n",
    "                n100, r10, r20, r50))\n",
    "    print('-' * 89)\n",
    "\n",
    "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
    "\n",
    "\n",
    "    # Save the model if the r10 is the best we've seen so far.\n",
    "    if r10 > best_r10:\n",
    "        with open(args.dae_save, 'wb') as f:\n",
    "            #torch.save({'state_dict': model_dae.state_dict()}, f)\n",
    "            torch.save(model_dae, f)\n",
    "        best_r10 = r10\n",
    "        print(\"Better performance! save best model...\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"n100\": n100,\n",
    "        \"r10\": r10,\n",
    "        \"r20\": r20,\n",
    "        \"r50\": r50\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 887.203 | n100 0.443 | r10 0.368 | r20 0.343 | r50 0.41\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "# with open(args.dae_save, 'rb') as f:\n",
    "#     model_dae = torch.load(f)\n",
    "with open(args.dae_save, 'rb') as f:\n",
    "    # model_dae = MultiDAE(p_dims).to(device)\n",
    "    # checkpoint = torch.load(f)\n",
    "    # model_dae.load_state_dict(checkpoint['state_dict'])\n",
    "    model_dae = torch.load(f).to(device)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, n100, r10, r20, r50 = evaluate(model_dae, criterion, test_data_tr, test_data_te, is_VAE=False)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.3f} | n100 {:4.3f} | r10 {:4.3f} | r20 {:4.3f} | '\n",
    "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
    "print('=' * 89)\n",
    "wandb.watch(model_dae)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training Multi-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"movie_recommendation\", entity=\"glancyes\")\n",
    "wandb.config.update(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "\n",
    "n_items = loader.load_n_items()\n",
    "train_data = loader.load_data('train')\n",
    "vad_data_tr, vad_data_te = loader.load_data('validation')\n",
    "test_data_tr, test_data_te = loader.load_data('test')\n",
    "check_data_tr, check_data_te = loader.load_data('check')\n",
    "\n",
    "N = train_data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "p_dims = [200, 3000, n_items]\n",
    "model_vae = MultiVAE(p_dims).to(device)\n",
    "\n",
    "optimizer2 = adabound.AdaBound(model_vae.parameters(), lr=1e-3, final_lr=0.1)\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=args.wd)\n",
    "#criterion2 = loss_function_vae\n",
    "criterion2 = loss_function_vae\n",
    "\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "r10_fin_list = []\n",
    "\n",
    "best_r10 = -np.inf\n",
    "update_count = 0\n",
    "new_epochs = args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3.18s | valid loss 973.47 | n100 0.343 | r10 0.279 | r20 0.256 | r50 0.312\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 3.09s | valid loss 956.59 | n100 0.380 | r10 0.323 | r20 0.293 | r50 0.343\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 3.08s | valid loss 943.04 | n100 0.400 | r10 0.337 | r20 0.308 | r50 0.362\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 3.07s | valid loss 936.05 | n100 0.414 | r10 0.349 | r20 0.317 | r50 0.376\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 3.14s | valid loss 930.15 | n100 0.424 | r10 0.364 | r20 0.328 | r50 0.385\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 3.13s | valid loss 926.01 | n100 0.427 | r10 0.358 | r20 0.329 | r50 0.389\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 3.20s | valid loss 921.97 | n100 0.435 | r10 0.371 | r20 0.339 | r50 0.395\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 3.02s | valid loss 918.61 | n100 0.438 | r10 0.375 | r20 0.342 | r50 0.401\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 3.05s | valid loss 915.17 | n100 0.437 | r10 0.368 | r20 0.340 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 3.17s | valid loss 912.14 | n100 0.439 | r10 0.369 | r20 0.343 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 3.21s | valid loss 909.43 | n100 0.442 | r10 0.375 | r20 0.344 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "Better performance! save best model...\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 3.11s | valid loss 906.80 | n100 0.440 | r10 0.371 | r20 0.342 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 3.15s | valid loss 905.14 | n100 0.440 | r10 0.370 | r20 0.340 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 3.14s | valid loss 902.95 | n100 0.439 | r10 0.362 | r20 0.336 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 3.18s | valid loss 901.34 | n100 0.442 | r10 0.372 | r20 0.341 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 3.11s | valid loss 899.42 | n100 0.440 | r10 0.366 | r20 0.338 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 3.15s | valid loss 897.82 | n100 0.439 | r10 0.364 | r20 0.338 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 3.18s | valid loss 896.32 | n100 0.439 | r10 0.365 | r20 0.340 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 3.09s | valid loss 895.28 | n100 0.439 | r10 0.368 | r20 0.338 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 3.08s | valid loss 894.12 | n100 0.441 | r10 0.373 | r20 0.341 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 3.09s | valid loss 893.19 | n100 0.440 | r10 0.368 | r20 0.339 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 3.15s | valid loss 891.97 | n100 0.439 | r10 0.363 | r20 0.337 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 3.14s | valid loss 891.25 | n100 0.439 | r10 0.365 | r20 0.336 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 3.23s | valid loss 890.93 | n100 0.439 | r10 0.368 | r20 0.338 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 3.18s | valid loss 890.09 | n100 0.437 | r10 0.362 | r20 0.334 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 3.20s | valid loss 888.90 | n100 0.436 | r10 0.357 | r20 0.335 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 3.09s | valid loss 888.89 | n100 0.438 | r10 0.367 | r20 0.338 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 3.07s | valid loss 887.95 | n100 0.436 | r10 0.360 | r20 0.336 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 3.07s | valid loss 887.96 | n100 0.437 | r10 0.365 | r20 0.337 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 3.12s | valid loss 887.34 | n100 0.440 | r10 0.368 | r20 0.338 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time: 3.14s | valid loss 886.78 | n100 0.439 | r10 0.362 | r20 0.336 | r50 0.404\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time: 3.15s | valid loss 886.31 | n100 0.436 | r10 0.359 | r20 0.333 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time: 3.21s | valid loss 885.98 | n100 0.437 | r10 0.359 | r20 0.335 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time: 3.24s | valid loss 885.45 | n100 0.439 | r10 0.364 | r20 0.335 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 3.16s | valid loss 885.56 | n100 0.437 | r10 0.361 | r20 0.334 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time: 3.22s | valid loss 885.18 | n100 0.438 | r10 0.361 | r20 0.336 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time: 3.12s | valid loss 885.07 | n100 0.439 | r10 0.366 | r20 0.336 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time: 3.09s | valid loss 885.32 | n100 0.437 | r10 0.364 | r20 0.335 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time: 3.06s | valid loss 885.31 | n100 0.438 | r10 0.365 | r20 0.336 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 3.23s | valid loss 884.89 | n100 0.438 | r10 0.365 | r20 0.337 | r50 0.403\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time: 3.03s | valid loss 883.72 | n100 0.439 | r10 0.363 | r20 0.336 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time: 3.11s | valid loss 884.33 | n100 0.435 | r10 0.358 | r20 0.333 | r50 0.400\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time: 3.19s | valid loss 883.96 | n100 0.437 | r10 0.361 | r20 0.334 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time: 3.11s | valid loss 883.37 | n100 0.438 | r10 0.363 | r20 0.339 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 3.11s | valid loss 883.53 | n100 0.439 | r10 0.362 | r20 0.336 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time: 3.12s | valid loss 883.79 | n100 0.442 | r10 0.368 | r20 0.339 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time: 3.16s | valid loss 883.87 | n100 0.436 | r10 0.359 | r20 0.335 | r50 0.402\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time: 3.15s | valid loss 883.33 | n100 0.440 | r10 0.365 | r20 0.338 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time: 3.28s | valid loss 882.60 | n100 0.439 | r10 0.362 | r20 0.338 | r50 0.406\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 3.14s | valid loss 883.19 | n100 0.438 | r10 0.361 | r20 0.337 | r50 0.405\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, new_epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_loss = train(model_vae, criterion2, optimizer2, epoch, is_VAE=True)\n",
    "    val_loss, n100, r10, r20, r50 = evaluate(model_vae, criterion2, vad_data_tr, vad_data_te, is_VAE=True)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:4.2f}s | valid loss {:4.2f} | '\n",
    "            'n100 {:5.3f} | r10 {:5.3f} | r20 {:5.3f} | r50 {:5.3f}'.format(\n",
    "                epoch, time.time() - epoch_start_time, val_loss,\n",
    "                n100, r10, r20, r50))\n",
    "    print('-' * 89)\n",
    "\n",
    "    n_iter = epoch * len(range(0, N, args.batch_size))\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    r10_fin_list.append(r10)\n",
    "\n",
    "    # Save the model if the r10 is the best we've seen so far.\n",
    "    if r10 > best_r10:\n",
    "        with open(args.vae_save, 'wb') as f:\n",
    "            # torch.save({'state_dict': model_vae.state_dict()}, f)\n",
    "            torch.save(model_vae, f)\n",
    "        best_r10 = r10\n",
    "        print(\"Better performance! save best model...\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"n100\": n100,\n",
    "        \"r10\": r10,\n",
    "        \"r20\": r20,\n",
    "        \"r50\": r50\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss 917.58 | n100 0.44 | r10 0.37 | r20 0.34 | r50 0.40\n",
      "=========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the best saved model.\n",
    "with open(args.vae_save, 'rb') as f:\n",
    "    # model_vae = MultiVAE(p_dims).to(device)\n",
    "    # checkpoint = torch.load(f)\n",
    "    # model_vae.load_state_dict(checkpoint['state_dict'])\n",
    "    model_vae = torch.load(f).to(device)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss, n100, r10, r20, r50 = evaluate(model_vae, criterion2, test_data_tr, test_data_te, is_VAE=True)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:4.2f} | n100 {:4.2f} | r10 {:4.2f} | r20 {:4.2f} | '\n",
    "        'r50 {:4.2f}'.format(test_loss, n100, r10, r20, r50))\n",
    "print('=' * 89)\n",
    "wandb.watch(model_vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x.mul(torch.sigmoid(x))\n",
    "\n",
    "def log_norm_pdf(x, mu, logvar):\n",
    "    return -0.5*(logvar + np.log(2 * np.pi) + (x - mu).pow(2) / logvar.exp())\n",
    "\n",
    "\n",
    "class CompositePrior(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, mixture_weights=[3/20, 3/4, 1/10]):\n",
    "        super(CompositePrior, self).__init__()\n",
    "\n",
    "        self.mixture_weights = mixture_weights\n",
    "\n",
    "        self.mu_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.mu_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_prior.data.fill_(0)\n",
    "\n",
    "        self.logvar_uniform_prior = nn.Parameter(torch.Tensor(1, latent_dim), requires_grad=False)\n",
    "        self.logvar_uniform_prior.data.fill_(10)\n",
    "\n",
    "        self.encoder_old = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.encoder_old.requires_grad_(False)\n",
    "\n",
    "    def forward(self, x, z):\n",
    "        post_mu, post_logvar = self.encoder_old(x, 0)\n",
    "\n",
    "        stnd_prior = log_norm_pdf(z, self.mu_prior, self.logvar_prior)\n",
    "        post_prior = log_norm_pdf(z, post_mu, post_logvar)\n",
    "        unif_prior = log_norm_pdf(z, self.mu_prior, self.logvar_uniform_prior)\n",
    "\n",
    "        gaussians = [stnd_prior, post_prior, unif_prior]\n",
    "        gaussians = [g.add(np.log(w)) for g, w in zip(gaussians, self.mixture_weights)]\n",
    "\n",
    "        density_per_gaussian = torch.stack(gaussians, dim=-1)\n",
    "\n",
    "        return torch.logsumexp(density_per_gaussian, dim=-1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim, eps=1e-1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln3 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln4 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ln5 = nn.LayerNorm(hidden_dim, eps=eps)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, dropout_rate):\n",
    "        norm = x.pow(2).sum(dim=-1).sqrt()\n",
    "        x = x / norm[:, None]\n",
    "\n",
    "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
    "\n",
    "        h1 = self.ln1(swish(self.fc1(x)))\n",
    "        h2 = self.ln2(swish(self.fc2(h1) + h1))\n",
    "        h3 = self.ln3(swish(self.fc3(h2) + h1 + h2))\n",
    "        h4 = self.ln4(swish(self.fc4(h3) + h1 + h2 + h3))\n",
    "        h5 = self.ln5(swish(self.fc5(h4) + h1 + h2 + h3 + h4))\n",
    "\n",
    "        return self.fc_mu(h5), self.fc_logvar(h5)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(hidden_dim, latent_dim, input_dim)\n",
    "        self.prior = CompositePrior(hidden_dim, latent_dim, input_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, user_ratings, beta=None, gamma=1, dropout_rate=0.5, calculate_loss=True):\n",
    "        mu, logvar = self.encoder(user_ratings, dropout_rate=dropout_rate)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_pred = self.decoder(z)\n",
    "\n",
    "        if calculate_loss:\n",
    "            if gamma:\n",
    "                norm = user_ratings.sum(dim=-1)\n",
    "                kl_weight = gamma * norm\n",
    "            elif beta:\n",
    "                kl_weight = beta\n",
    "\n",
    "            mll = (F.log_softmax(x_pred, dim=-1) * user_ratings).sum(dim=-1).mean()\n",
    "            kld = (log_norm_pdf(z, mu, logvar) - self.prior(user_ratings, z)).sum(dim=-1).mul(kl_weight).mean()\n",
    "            negative_elbo = -(mll - kld)\n",
    "\n",
    "            return (mll, kld), negative_elbo\n",
    "\n",
    "        else:\n",
    "            return x_pred\n",
    "\n",
    "    def update_prior(self):\n",
    "        self.prior.encoder_old.load_state_dict(deepcopy(self.encoder.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_kwargs = {\n",
    "    'hidden_dim': 600,\n",
    "    'latent_dim': 200,\n",
    "    'input_dim': n_items\n",
    "}\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(args.recvae_save, 'rb') as f:\n",
    "    model_recvae = VAE(**model_kwargs).to(device)\n",
    "    checkpoint = torch.load(f)\n",
    "    model_recvae.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## 배치사이즈 포함\n",
    "def numerize_for_infer(tp, profile2id, show2id):\n",
    "    uid = tp['user'].apply(lambda x: profile2id[x])\n",
    "    sid = tp['item'].apply(lambda x: show2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 준비\n",
    "torch.cuda.empty_cache()\n",
    "infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "n_items = loader.load_n_items()\n",
    "\n",
    "n_users = infer_df['uid'].max() + 1\n",
    "\n",
    "rows, cols = infer_df['uid'], infer_df['sid']\n",
    "data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, n_items))\n",
    "\n",
    "N = data.shape[0]\n",
    "idxlist = list(range(N))\n",
    "\n",
    "model_recvae.eval()\n",
    "model_dae.eval()\n",
    "model_vae.eval()\n",
    "total_loss = 0.0\n",
    "e_idxlist = list(range(data.shape[0]))\n",
    "e_N = data.shape[0]\n",
    "pred_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31360, 10)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_loss2 = 0\n",
    "with torch.no_grad():\n",
    "    for start_idx in range(0, e_N, args.batch_size):\n",
    "        end_idx = min(start_idx + args.batch_size, N)\n",
    "        data_batch = data[e_idxlist[start_idx:end_idx]]\n",
    "\n",
    "        data_tensor = naive_sparse2tensor(data_batch).to(device)\n",
    "        data_tensor2 = naive_sparse2tensor(data_batch).to(device)\n",
    "\n",
    "        if args.total_anneal_steps > 0:\n",
    "            anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
    "        else:\n",
    "            anneal = args.anneal_cap\n",
    "\n",
    "        # recon_batch = model_dae(data_tensor)\n",
    "        # loss = criterion(recon_batch, data_tensor)\n",
    "\n",
    "        #recon_genre, recon_batch = model_dae(data_tensor)\n",
    "        #loss = criterion(recon_genre, recon_batch, data_tensor)\n",
    "\n",
    "        recon_genre, recon_title, recon_batch = model_dae(data_tensor)\n",
    "\n",
    "\n",
    "        # recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "        # loss2 = criterion2(recon_batch2, data_tensor2, mu, logvar, anneal)\n",
    "\n",
    "        # recon_genre2, recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "        # loss2 = criterion2(recon_genre2, recon_batch2, data_tensor2, mu, logvar, anneal)\n",
    "\n",
    "        recon_genre2, recon_title2, recon_batch2, mu, logvar = model_vae(data_tensor2)\n",
    "\n",
    "        recon_batch3 = model_recvae(data_tensor, calculate_loss=False)\n",
    "\n",
    "\n",
    "        # Exclude examples from training set\n",
    "        recon_batch = recon_batch.cpu().numpy()\n",
    "        recon_batch2 = recon_batch2.cpu().numpy()\n",
    "        recon_batch3 = recon_batch3.cpu().numpy()\n",
    "\n",
    "\n",
    "        recon_batch = np.add(recon_batch, recon_batch2) # 1:1로 앙상블\n",
    "        recon_batch = np.add(recon_batch, recon_batch3)\n",
    "\n",
    "        recon_batch[data_batch.nonzero()] = -np.inf\n",
    "\n",
    "        ##Recall\n",
    "        batch_users = recon_batch.shape[0]\n",
    "        idx = bn.argpartition(-recon_batch, 10, axis=1)[:, :10]\n",
    "        if start_idx == 0:\n",
    "            pred_list = idx\n",
    "        else:\n",
    "            pred_list = np.append(pred_list, idx, axis=0)\n",
    "\n",
    "print(pred_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## sample_submission에 맞게끔 바꾸기\n",
    "user2 = []\n",
    "item2 = []\n",
    "for i_idx, arr_10 in enumerate(pred_list):\n",
    "    user2.extend([i_idx]*10)\n",
    "    item2.extend(arr_10)\n",
    "\n",
    "u2 = pd.DataFrame(user2, columns=['user'])\n",
    "i2 = pd.DataFrame(item2, columns=['item'])\n",
    "all2 = pd.concat([u2, i2], axis=1)\n",
    "\n",
    "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
    "re_s2id = dict((v, k) for k, v in show2id.items())\n",
    "\n",
    "def de_numerize(tp, re_p2id, re_s2id):\n",
    "    uid2 = tp['user'].apply(lambda x: re_p2id[x])\n",
    "    sid2 = tp['item'].apply(lambda x: re_s2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid2, 'sid': sid2}, columns=['uid', 'sid'])\n",
    "\n",
    "ans2 = de_numerize(all2, re_p2id, re_s2id)\n",
    "ans2.columns = ['user', 'item']\n",
    "new_ans2 = ans2.sort_values('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313600"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 확인용\n",
    "submit_data = pd.read_csv('/opt/ml/input/data/eval/sample_submission.csv', sep='\\t')\n",
    "sum(new_ans2.user.values == submit_data.user.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "new_ans2.reset_index(drop=True, inplace=True)\n",
    "new_ans2.to_csv('ae_genre_title_embedding_skip_connection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318     5309\n",
      "2571    4548\n",
      "356     4520\n",
      "2959    4320\n",
      "2858    4258\n",
      "        ... \n",
      "1948       1\n",
      "3997       1\n",
      "2099       1\n",
      "4255       1\n",
      "3202       1\n",
      "Name: item, Length: 4269, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(new_ans2['item'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc04c06d6d0>]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ4UlEQVR4nO3df2wc533n8fd3dpc/RFISZdG0K8mRFMvxKUFrG4qsnNNcap9l2U0j45C2ugsaISeccFcHSHEH9OzroUaSBkgOaNOkP1LoYuHkoKntS1JINdxzdLZzadzYFh3bin/EESXbkRTFpET9pCSSS37vj3mWnOUPcSktd8mZzwsgduaZZ2efeQB9dvTMszPm7oiISDZE9W6AiIjUjkJfRCRDFPoiIhmi0BcRyRCFvohIhuTr3YBLWbp0qa9cubLezRARmVdefPHF4+7eMdm2OR36K1eupKurq97NEBGZV8zsnam2aXhHRCRDFPoiIhmi0BcRyRCFvohIhij0RUQyRKEvIpIhCn0RkQxJZegfO32BP/3emxzqPVfvpoiIzCmpDP13zwzwF0938/aJ/no3RURkTkll6EcWv+r5MCIi5VIZ+kac+iMKfRGRMukM/dEzfaW+iEhSKkO/RJEvIlIulaEfhVN9neiLiJRLZehreEdEZHLpDv36NkNEZM5JZ+ij4R0RkclUFPpm9raZ/cTMXjazrlC2xMz2mtmB8Noeys3MvmZm3Wa238xuSexna6h/wMy2zs4hJebp61xfRKTMTM70f8Pdb3L3dWH9fuApd18DPBXWAe4G1oS/7cDXIf6SAB4EbgXWAw+WviiqrTS8o3n6IiLlrmR4ZzOwKyzvAu5NlD/sseeAxWZ2LXAXsNfd+9z9JLAX2HQFn38JpeEdpb6ISFKloe/A98zsRTPbHso63f1YWP4l0BmWlwGHE+89EsqmKi9jZtvNrMvMunp7eytsXrnS8I6IiJTLV1jvw+5+1MyuBvaa2U+TG93dzawqp9XuvgPYAbBu3brL2qdZ6TYMOtMXEUmq6Ezf3Y+G1x7g74nH5N8NwzaE155Q/SiwIvH25aFsqvKqK53oK/NFRMpNG/pm1mJmbaVlYCPwKrAHKM3A2QrsDst7gE+FWTwbgNNhGOhJYKOZtYcLuBtDWdWZ7rIpIjKpSoZ3OoG/D0MmeeBb7v5/zGwf8JiZbQPeAX4n1H8CuAfoBs4DnwZw9z4z+wKwL9T7vLv3Ve1IEkZvwzAbOxcRmcemDX13PwT82iTlJ4A7Jil34L4p9rUT2DnzZl4ejemLiJRL5y9yRwf169oMEZE5J6WhXxreUeqLiCSlMvT1uEQRkcmlMvT1uEQRkcmlM/R1wzURkUmlM/TDq4Z3RETKpTP0NU9fRGRSKQ39+FV32RQRKZfO0A+vynwRkXLpDH3T/fRFRCaTytCP9GB0EZFJpTL0NU9fRGRyqQz9KBzViFJfRKRMOkNfT84SEZlUKkM/Fwb1hxX6IiJlUhn6o2f6Gt4RESmTytAfPdMfqXNDRETmmFSGfmnKpoZ3RETKpTL0zYzINLwjIjJeKkMf4iEenemLiJRLbehHZjrTFxEZJ7Whn4uMYYW+iEiZ9Ia+aXhHRGS81IZ+FJlurSwiMk56Q9/Q8I6IyDipDX3N3hERmSi1oa/ZOyIiE6U29DV7R0RkoopD38xyZvaSmT0e1leZ2fNm1m1mj5pZQyhvDOvdYfvKxD4eCOVvmtldVT+ahEizd0REJpjJmf5ngTcS618GvuLu1wMngW2hfBtwMpR/JdTDzNYCW4D3A5uAvzaz3JU1f2q5SMM7IiLjVRT6ZrYc+E3gG2HdgNuBb4cqu4B7w/LmsE7Yfkeovxl4xN0H3P0toBtYX4VjmFR8IXe29i4iMj9Veqb/58AfAqWbFV8FnHL3Ylg/AiwLy8uAwwBh++lQf7R8kveMMrPtZtZlZl29vb2VH8k4uuGaiMhE04a+mX0M6HH3F2vQHtx9h7uvc/d1HR0dl70fXcgVEZkoX0Gd24CPm9k9QBOwEPgqsNjM8uFsfjlwNNQ/CqwAjphZHlgEnEiUlyTfU3W6kCsiMtG0Z/ru/oC7L3f3lcQXYp92908CzwCfCNW2ArvD8p6wTtj+tLt7KN8SZvesAtYAL1TtSMaJzHCFvohImUrO9KfyX4FHzOxPgJeAh0L5Q8A3zawb6CP+osDdXzOzx4DXgSJwn7sPX8HnX5KGd0REJppR6Lv794Hvh+VDTDL7xt0vAr89xfu/CHxxpo28HJFm74iITJDeX+Rq9o6IyATpDX0N74iITJDa0NfsHRGRiVIb+roNg4jIRKkOfZ3pi4iUS23o6376IiITpTb0daYvIjJRakM/MmN4ZPp6IiJZkuLQR7dhEBEZJ7Whr3n6IiITpTb0I43pi4hMkNrQz2n2jojIBOkNfZ3pi4hMkNrQj+fp17sVIiJzS2pDPxehC7kiIuOkOPQ1vCMiMl5qQ1+3YRARmSi1oa8zfRGRiVIb+vFtGBT6IiJJqQ59neiLiJRLbehr9o6IyESpDX3dhkFEZKLUhr5uwyAiMlF6Q19n+iIiE6Q29EsXcnVPfRGRMakN/VxkgC7miogkpT/0daYvIjIqtaEfWRz6utOmiMiYaUPfzJrM7AUze8XMXjOzz4XyVWb2vJl1m9mjZtYQyhvDenfYvjKxrwdC+ZtmdtesHRXxPH3Qmb6ISFIlZ/oDwO3u/mvATcAmM9sAfBn4irtfD5wEtoX624CTofwroR5mthbYArwf2AT8tZnlqngsZUpn+hrTFxEZM23oe+xcWC2EPwduB74dyncB94blzWGdsP0OM7NQ/oi7D7j7W0A3sL4aBzGZUuhr9o6IyJiKxvTNLGdmLwM9wF7gIHDK3YuhyhFgWVheBhwGCNtPA1clyyd5T/KztptZl5l19fb2zviASpoK8X8iLgwNX/Y+RETSpqLQd/dhd78JWE58dn7jbDXI3Xe4+zp3X9fR0XHZ+1nUXADgzIXiNDVFRLJjRrN33P0U8AzwIWCxmeXDpuXA0bB8FFgBELYvAk4kyyd5T9UtbI6bdubi0Gx9hIjIvFPJ7J0OM1sclpuBO4E3iMP/E6HaVmB3WN4T1gnbn/Z4YH0PsCXM7lkFrAFeqNJxTNDaGIf+WYW+iMio/PRVuBbYFWbaRMBj7v64mb0OPGJmfwK8BDwU6j8EfNPMuoE+4hk7uPtrZvYY8DpQBO5z91kbcC+EOZtDw7qQKyJSMm3ou/t+4OZJyg8xyewbd78I/PYU+/oi8MWZN3Pm8jlN2RQRGS+1v8jNR/GhFRX6IiKjUhz68Zl+cVj3YRARKUlv6IfhHZ3pi4iMSW/ol4Z3dCFXRGRUekN/9EKuhndEREpSG/ql2zCcHdAvckVESlIb+q2NefKR0a/QFxEZldrQh/gHWvpxlojImFSHfj5nDBY1pi8iUpLq0G/IRQxpnr6IyKhUh34hF2nKpohIQrpDP2860xcRSUh36OciBhX6IiKjUh36GtMXESmX6tDP50xTNkVEElId+oVcpCmbIiIJqQ99De+IiIxJdeg35hX6IiJJqQ593YZBRKRcykNf8/RFRJJSHvqapy8ikpTq0Nc8fRGRcqkO/UIuYqioMX0RkZJ0h77uvSMiUibdoa8xfRGRMqkOfY3pi4iUS3Xo6zYMIiLlUh36rU15RhzOXhyqd1NEROaEaUPfzFaY2TNm9rqZvWZmnw3lS8xsr5kdCK/todzM7Gtm1m1m+83slsS+tob6B8xs6+wdVmxJSwMAp84r9EVEoLIz/SLwX9x9LbABuM/M1gL3A0+5+xrgqbAOcDewJvxtB74O8ZcE8CBwK7AeeLD0RTFb2hrzAJy9WJzNjxERmTemDX13P+buPw7LZ4E3gGXAZmBXqLYLuDcsbwYe9thzwGIzuxa4C9jr7n3ufhLYC2yq5sGM19ZUAODcgEJfRARmOKZvZiuBm4HngU53PxY2/RLoDMvLgMOJtx0JZVOVj/+M7WbWZWZdvb29M2neBIsXxKF//NzAFe1HRCQtKg59M2sFvgP8gbufSW5zdweq8tNXd9/h7uvcfV1HR8cV7evqtkYA+voHq9E0EZF5r6LQN7MCceD/rbt/NxS/G4ZtCK89ofwosCLx9uWhbKryWdMSxvT7NbwjIgJUNnvHgIeAN9z9zxKb9gClGThbgd2J8k+FWTwbgNNhGOhJYKOZtYcLuBtD2axZ0JCjIR/Re1bDOyIiAPkK6twG/B7wEzN7OZT9N+BLwGNmtg14B/idsO0J4B6gGzgPfBrA3fvM7AvAvlDv8+7eV42DmIqZsXppC2+fOD+bHyMiMm9MG/ru/kPApth8xyT1Hbhvin3tBHbOpIFXamFzQT/OEhEJUv2LXICWhhznB4fr3QwRkTkh9aG/oDFP/6Au5IqIQAZCv6Uhx/kBnemLiEAGQn9Bg870RURKUh/6rY15zg8OE19fFhHJttSH/oLGHMMjzoDuqy8ikv7Qb2mIZ6VqBo+ISAZCf0FDDtCtGEREIAOh3xFuunbs9MU6t0REpP5SH/qlp2edvqBf5YqIpD70FzXH99RX6IuIKPRFRDIl9aFfemSiQl9EJAOhn4uMtqY8ZxT6IiLpD32Ih3h0pi8iotAXEckUhb6ISIYo9EVEMkShLyKSIZkKfd1eWUSyLhOh37mwicHiCH39g/VuiohIXWUi9BeGX+X267GJIpJxmQj9ltLtlfXYRBHJuEyEvm7FICISy0Tot7fEoX/qvEJfRLItE6FfutPmmYsKfRHJtkyEfvuC+EEqJ85p9o6IZFsmQr+lMU/7ggKHT56vd1NEROoqE6EPsGLJAg73KfRFJNumDX0z22lmPWb2aqJsiZntNbMD4bU9lJuZfc3Mus1sv5ndknjP1lD/gJltnZ3Dmdp1SxZwqLdfv8oVkUyr5Ez/fwGbxpXdDzzl7muAp8I6wN3AmvC3Hfg6xF8SwIPArcB64MHSF0WtrF+1hKOnLnDk5IVafqyIyJwybei7+w+AvnHFm4FdYXkXcG+i/GGPPQcsNrNrgbuAve7e5+4ngb1M/CKZVe/rbAPg0PH+Wn6siMiccrlj+p3ufiws/xLoDMvLgMOJekdC2VTlE5jZdjPrMrOu3t7ey2zeRCuXtgDwxrEzVduniMh8c8UXcj0eJK/aQLm773D3de6+rqOjo1q7pXNhE6uWtvBs9/Gq7VNEZL653NB/NwzbEF57QvlRYEWi3vJQNlV5Td39gWv4pwPHdTsGEcmsyw39PUBpBs5WYHei/FNhFs8G4HQYBnoS2Ghm7eEC7sZQVlO3Xb8UgH/8ybFpaoqIpFMlUzb/DvgR8D4zO2Jm24AvAXea2QHgX4d1gCeAQ0A38D+B3wdw9z7gC8C+8Pf5UFZTH1p9Fc2FHM8ePFHrjxYRmRPy01Vw9387xaY7JqnrwH1T7GcnsHNGrauyKDLuXNvJ82+dwN0xs3o2R0Sk5jLzi9yS9auW8O6ZAd45oV/nikj2ZC70N6xeAsCzBzWLR0SyJ3Oh/96OVpa2NvD/3qzebwBEROaLzIW+mXHb9Uv554MnGB7RfXhEJFsyF/oAv76mg3MDRbp7ztW7KSIiNZXJ0P/V5YsAeOXIqfo2RESkxjIZ+tctWQBA79mBOrdERKS2Mhn6TYUci5oLHOrVHTdFJFsyGfoAH7mhg3860KuHqohIpmQ29NevWkLP2QEO9+mhKiKSHdkN/ZXxj7ReeLvmtwASEambzIb+mqtbWdRcYN9bCn0RyY7Mhn4UGR9c2c4+nemLSIZkNvQBbr6unUPH+/nFKY3ri0g2ZDr0/9UNHeQi40+/97N6N0VEpCYyHfofWLaI39vwHna/fJQjJ3WrZRFJv0yHPsC2D68iFxm/9Rc/5PH9v6h3c0REZlXmQ3/FkgXs/sxtLG9fwGe+9RLPdus++yKSXpkPfYAbr1nII9s3sPKqBdz/3f26J4+IpJZCP2hpzPOFez/AsVMX+d0dP+LMxaF6N0lEpOoU+gm/vqaDh7et5+cnzvMfv/ki5weL9W6SiEhVKfTH+ZfvXcrnNr+ffz54go//5bM8d+hEvZskIlI1Cv1JfPLW9/CX/+5mzlwYYsuO5/jj3a/Sc/ZivZslInLFbC7fWnjdunXe1dVVt8+/ODTM5/7hdR7d93MKuYjf/eAKNt/0K9y0op1cZHVrl4jIpZjZi+6+btJtCv3pvX28n69//yDf+fERiiPONQub+A8fWc0HV7ZzQ2cbTYVcvZsoIjJKoV8lPWcv8qODJ3joh2+x/8hpANoa8/zGjVfz0fd1cOM1C7nxmjYi/S9AROpIoV9l7s7P+87z+i/O8NRPe3j6pz309Q8CsLS1gX9x7UJWL23h/b+yiM5FTaxe2sKyxc36MhCRmlDoz7LhEedg7zlePXqaH/ysl4O9/RzsPcf5weHROouaC9zQ2co1i5rpbGtkSWsDyxY3s2xxM50Lm1jYXKC1Ma9rBSJyxS4V+vk6NGYT8FUgB3zD3b9U6zZUWy4ybuhs44bONv7NLcsBGBlx3jrRz4lzgxzoOcurR89w4N2zvHL4FMfPDZR9IZSYxcNFS1oaWNRcYGFzgUXNBRrzORoLEW2NeRoLORY05GhtzNPamKetKU9zIUdbU4GGfERjPqK1KU8hFy835iPM9EUiIrGahr6Z5YC/Au4EjgD7zGyPu79ey3bUQhQZ7+1o5b0d8fN4x+sfKHLs9AUO912g99wAZy8WOXV+kNMXhjh1fohTF4Y4fWGIIycvMFgcYaA4zJmLRQaLIzNqRy4yGnIRhZzR3JCjMZ8jHxlRZOQjI58zmgs5cpGRjyJykZGL4rJCLq6Xs7gsCvtqLETkzIhsbHtk8TE3FcL+w3pU2hbq53NGYz5HZGBmGBBFYBgWyiJLrsfLDfmIfGSj66XvMQv7TpZbKKdsPf6shnxEPhe/OVmfRJ3SfkfXjLHPC/XGlinbF+PeX+pPkbmi1mf664Fudz8EYGaPAJuB1IX+dFoa81x/dRvXX902o/eNjDgXhobpHyhydqBI/0CRcwNF+geGGSyOcHFomPODRQaKIwwUR+gfKFIccQYTy8URZ3hkhOKwMzQ8wsWhEYZHnAvDw6Pbzg+Ulp0RH3sdKI4wWBzBHYZDuVxaMvSTXypjZVa+cZp6Nkm9uNwmlE2y67F6k37e1G3NRUYhN/1Peyr9j6UxfcVK9lXJx1Xyv92Kv5pr1KaP3tDBf//Y2sraNAO1Dv1lwOHE+hHg1mQFM9sObAe47rrrateyeSKKjJbGPC2Nea6ud2MCD+E/7M6FweHwBRGXj4Qvh5ERxx0Gh4cZCF8a7uCM1fWwL3fK3u/EXzalfYzWI+wjuUx5HcaVDxTjLzgPG3z0GBL1w/rYsieOtVTu4+rEZck6AIPh80rvGb89+fljZV6+sazeZG1hQtlUnzdWL7GfCt87NDxCcbov+QrPASqpVsn1xsr2U0GdCvYT76s6baqk0rWLmyvZ04zVfEx/Ou6+A9gB8YXcOjdHKmBh2CYPNOb1mwWRuazWt2E4CqxIrC8PZSIiUgO1Dv19wBozW2VmDcAWYE+N2yAiklk1Hd5x96KZfQZ4knjK5k53f62WbRARybKaj+m7+xPAE7X+XBER0a2VRUQyRaEvIpIhCn0RkQxR6IuIZMicvsummfUC71zBLpYCx6vUnLRR30xNfTM19c2lzZX+eY+7d0y2YU6H/pUys66pbi+adeqbqalvpqa+ubT50D8a3hERyRCFvohIhqQ99HfUuwFzmPpmauqbqalvLm3O90+qx/RFRKRc2s/0RUQkQaEvIpIhqQx9M9tkZm+aWbeZ3V/v9tSCme00sx4zezVRtsTM9prZgfDaHsrNzL4W+me/md2SeM/WUP+AmW2tx7FUm5mtMLNnzOx1M3vNzD4bytU/gJk1mdkLZvZK6J/PhfJVZvZ86IdHw+3QMbPGsN4dtq9M7OuBUP6mmd1Vp0OqOjPLmdlLZvZ4WJ+/fRM/ni49f8S3bD4IrAYagFeAtfVuVw2O+yPALcCribL/Adwflu8HvhyW7wH+kfhRnhuA50P5EuBQeG0Py+31PrYq9M21wC1huQ34GbBW/TPaPwa0huUC8Hw47seALaH8b4D/FJZ/H/ibsLwFeDQsrw3/3hqBVeHfYa7ex1elPvrPwLeAx8P6vO2bNJ7pjz583d0HgdLD11PN3X8A9I0r3gzsCsu7gHsT5Q977DlgsZldC9wF7HX3Pnc/CewFNs1642eZux9z9x+H5bPAG8TPa1b/AOE4z4XVQvhz4Hbg26F8fP+U+u3bwB0WP+V7M/CIuw+4+1tAN/G/x3nNzJYDvwl8I6wb87hv0hj6kz18fVmd2lJvne5+LCz/EugMy1P1Uer7Lvx3+2bis1n1TxCGL14Geoi/zA4Cp9y9GKokj3W0H8L208BVpLd//hz4Q2AkrF/FPO6bNIa+TMLj/2Nmen6umbUC3wH+wN3PJLdlvX/cfdjdbyJ+bvV64Mb6tmhuMLOPAT3u/mK921ItaQx9PXx9zLthWILw2hPKp+qj1PadmRWIA/9v3f27oVj9M467nwKeAT5EPKxVerpe8lhH+yFsXwScIJ39cxvwcTN7m3io+Hbgq8zjvklj6Ovh62P2AKUZJluB3YnyT4VZKhuA02GY40lgo5m1h5ksG0PZvBbGVB8C3nD3P0tsUv8AZtZhZovDcjNwJ/F1j2eAT4Rq4/un1G+fAJ4O/1PaA2wJM1hWAWuAF2pyELPE3R9w9+XuvpI4S552908yn/um3lfFZ+OPePbFz4jHJf+o3u2p0TH/HXAMGCIeL9xGPJb4FHAA+L/AklDXgL8K/fMTYF1iP/+e+CJTN/Dpeh9Xlfrmw8RDN/uBl8PfPeqf0WP6VeCl0D+vAn8cylcTB1M38L+BxlDeFNa7w/bViX39Uei3N4G7631sVe6njzI2e2fe9o1uwyAikiFpHN4REZEpKPRFRDJEoS8ikiEKfRGRDFHoi4hkiEJfRCRDFPoiIhny/wHMv0GKFjRi6gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(new_ans2['item'].value_counts().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 유저군 분석을 위한 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### 데이터 준비\n",
    "infer_df = numerize_for_infer(raw_data, profile2id, show2id)\n",
    "\n",
    "loader = DataLoader(args.data)\n",
    "n_items = loader.load_n_items()\n",
    "\n",
    "n_users = infer_df['uid'].max() + 1\n",
    "\n",
    "rows, cols = infer_df['uid'], infer_df['sid']\n",
    "data = sparse.csr_matrix((np.ones_like(rows),\n",
    "                                 (rows, cols)), dtype='float64',\n",
    "                                 shape=(n_users, n_items))\n",
    "\n",
    "N = data.shape[0]\n",
    "idxlist = list(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(args.dae_save, 'rb') as f:\n",
    "    model_dae = MultiDAE(p_dims).to(device)\n",
    "    checkpoint = torch.load(f)\n",
    "    model_dae.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(args.vae_save, 'rb') as f:\n",
    "    model_vae = MultiVAE(p_dims).to(device)\n",
    "    checkpoint = torch.load(f)\n",
    "    model_vae.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = loss_function_dae_genre\n",
    "criterion2 = loss_function_vae_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "total_loss2 = 0\n",
    "update_count = 0 #tmp\n",
    "\n",
    "model_dae.eval()\n",
    "model_vae.eval()\n",
    "with torch.no_grad():\n",
    "    data_tensor = naive_sparse2tensor(check_data_tr).to(device)\n",
    "    data_tensor2 = naive_sparse2tensor(check_data_tr).to(device)\n",
    "\n",
    "    if args.total_anneal_steps > 0:\n",
    "        anneal = min(args.anneal_cap, 1. * update_count / args.total_anneal_steps)\n",
    "    else:\n",
    "        anneal = args.anneal_cap\n",
    "\n",
    "    recon_genre, recon_title, dae_ori_output = model_dae(data_tensor)\n",
    "    loss = criterion(recon_genre, recon_title, dae_ori_output, data_tensor)\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    recon_genre2, recon_title2, vae_ori_output, mu, logvar = model_vae(data_tensor2)\n",
    "    loss2 = criterion2(recon_genre2, recon_title2, vae_ori_output, data_tensor2, mu, logvar, anneal)\n",
    "    total_loss2 += loss2.item()\n",
    "\n",
    "    # Exclude examples from training set\n",
    "    dae_ori_output = dae_ori_output.cpu().numpy()\n",
    "    vae_ori_output = vae_ori_output.cpu().numpy()\n",
    "\n",
    "    recon_genre = recon_genre.cpu().numpy()\n",
    "    recon_genre2 = recon_genre2.cpu().numpy()\n",
    "\n",
    "    recon_title = recon_title.cpu().numpy()\n",
    "    recon_title2 = recon_title2.cpu().numpy()\n",
    "\n",
    "    ensemble_output = np.add(dae_ori_output, vae_ori_output) # 1:1로 앙상블\n",
    "    genre_add_ensemble_output = ensemble_output + np.mean(recon_genre)*10 + np.mean(recon_genre2)*10 + np.mean(recon_title)*10 + np.mean(recon_title2)*10\n",
    "\n",
    "    genre_add_ensemble_output[check_data_tr.nonzero()] = -np.inf\n",
    "    dae_ori_output[check_data_tr.nonzero()] = -np.inf\n",
    "    vae_ori_output[check_data_tr.nonzero()] = -np.inf\n",
    "\n",
    "    ensem_r10 = Recall_at_k_batch(genre_add_ensemble_output, check_data_te, 10)\n",
    "    dae_r10 = Recall_at_k_batch(dae_ori_output, check_data_te, 10)\n",
    "    vae_r10 = Recall_at_k_batch(vae_ori_output, check_data_te, 10)\n",
    "\n",
    "    ensem_r30 = Recall_at_k_batch(genre_add_ensemble_output, check_data_te, 30)\n",
    "    dae_r30 = Recall_at_k_batch(dae_ori_output, check_data_te, 30)\n",
    "    vae_r30 = Recall_at_k_batch(vae_ori_output, check_data_te, 30)\n",
    "\n",
    "    print('| ensem_r10 {:4.2f} | dae_r10 {:4.2f} | vae_r10 {:4.2f} | ensem_r30 {:4.2f} | '\n",
    "        'dae_r30 {:4.2f} | vae_r30 {:4.2f} '.format(np.mean(ensem_r10), np.mean(dae_r10), np.mean(vae_r10),\n",
    "         np.mean(ensem_r30), np.mean(dae_r30), np.mean(vae_r30)))\n",
    "\n",
    "    ##Recall\n",
    "    gen_emb_10_idx = bn.argpartition(-genre_add_ensemble_output, 10, axis=1)[:, :10]\n",
    "    gen_emb_30_idx = bn.argpartition(-genre_add_ensemble_output, 30, axis=1)[:, :30]\n",
    "\n",
    "    dae_ori_output_10_idx = bn.argpartition(-dae_ori_output, 10, axis=1)[:, :10]\n",
    "    dae_ori_output_30_idx = bn.argpartition(-dae_ori_output, 30, axis=1)[:, :30]\n",
    "\n",
    "    vae_ori_output_10_idx = bn.argpartition(-vae_ori_output, 10, axis=1)[:, :10]\n",
    "    vae_ori_output_30_idx = bn.argpartition(-vae_ori_output, 30, axis=1)[:, :30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def output_to_dataframe(idx):\n",
    "    user = []\n",
    "    item = []\n",
    "    if idx.shape[1] == 10:\n",
    "        for i_idx, arr_10 in enumerate(idx):\n",
    "            user.extend([i_idx]*10)\n",
    "            item.extend(arr_10)\n",
    "    else:\n",
    "        for i_idx, arr_10 in enumerate(idx):\n",
    "            user.extend([i_idx]*30)\n",
    "            item.extend(arr_10)\n",
    "\n",
    "    u = pd.DataFrame(user, columns=['user'])\n",
    "    i = pd.DataFrame(item, columns=['item'])\n",
    "    all = pd.concat([u, i], axis=1)\n",
    "\n",
    "    re_p2id = dict((v, k) for k, v in profile2id.items())\n",
    "    re_s2id = dict((v, k) for k, v in show2id.items())\n",
    "\n",
    "    def de_numerize(tp, re_p2id, re_s2id):\n",
    "        uid = tp['user'].apply(lambda x: re_p2id[x])\n",
    "        sid = tp['item'].apply(lambda x: re_s2id[x])\n",
    "        return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "    ans = de_numerize(all, re_p2id, re_s2id)\n",
    "    ans.columns = ['user', 'item']\n",
    "    new_ans = ans.sort_values('user')\n",
    "    return new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen_emb_10_result = output_to_dataframe(gen_emb_10_idx)\n",
    "gen_emb_30_result = output_to_dataframe(gen_emb_30_idx)\n",
    "\n",
    "dae_ori_output_10_result = output_to_dataframe(dae_ori_output_10_idx)\n",
    "dae_ori_output_30_result = output_to_dataframe(dae_ori_output_30_idx)\n",
    "\n",
    "vae_ori_output_10_result = output_to_dataframe(vae_ori_output_10_idx)\n",
    "vae_ori_output_30_result = output_to_dataframe(vae_ori_output_30_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen_emb_30_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gen_emb_10_result.to_csv('fin_gen_emb_10_result.csv', index=False)\n",
    "gen_emb_30_result.to_csv('fin_gen_emb_30_result.csv', index=False)\n",
    "\n",
    "dae_ori_output_10_result.to_csv('fin_dae_ori_output_10_result.csv', index=False)\n",
    "dae_ori_output_30_result.to_csv('fin_dae_ori_output_30_result.csv', index=False)\n",
    "\n",
    "vae_ori_output_10_result.to_csv('fin_vae_ori_output_10_result.csv', index=False)\n",
    "vae_ori_output_30_result.to_csv('fin_vae_ori_output_30_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(check_data_te.toarray())\n",
    "\n",
    "cols = df.columns.values\n",
    "mask = df.gt(0.0).values\n",
    "out = [cols[x].tolist() for x in mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "user = []\n",
    "item = []\n",
    "\n",
    "for i in range(len(out)):\n",
    "    user.extend([i]*len(out[i]))\n",
    "    item.extend(out[i])\n",
    "\n",
    "u = pd.DataFrame(user, columns=['user'])\n",
    "i = pd.DataFrame(item, columns=['item'])\n",
    "all = pd.concat([u, i], axis=1)\n",
    "\n",
    "re_p2id = dict((v, k) for k, v in profile2id.items())\n",
    "re_s2id = dict((v, k) for k, v in show2id.items())\n",
    "\n",
    "def de_numerize(tp, re_p2id, re_s2id):\n",
    "    uid = tp['user'].apply(lambda x: re_p2id[x])\n",
    "    sid = tp['item'].apply(lambda x: re_s2id[x])\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])\n",
    "\n",
    "real_ans = de_numerize(all, re_p2id, re_s2id)\n",
    "real_ans.columns = ['user', 'item']\n",
    "real_new_ans = real_ans.sort_values('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_new_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "real_new_ans.to_csv('real_new_ans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
